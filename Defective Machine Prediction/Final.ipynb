{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing # for standardizing numerical variables\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import collections\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Train & Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_data = pd.read_csv('F:\\\\Noodle.ai\\\\ManufacturingQuality\\\\TrainingSet.csv',header=0, encoding='UTF-8') \n",
    "Test_data = pd.read_csv('F:\\\\Noodle.ai\\\\ManufacturingQuality\\\\TestingSet.csv',header=0, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering \"Bad\" as positive and \"Good\" as Negative because need to build a model that can detect a bad device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_FP = 500 # cost of testing a good device classified as bad is $500\n",
    "cost_FN = 5000 # cost of testing a bad device classified as good is $5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking dimensions of Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dim  (3722, 220)\n",
      "Test dim  (400, 220)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dim \" , Train_data.shape)\n",
    "print(\"Test dim \" , Test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking datatypes of Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datatypes  V1               float64\n",
      "V2               float64\n",
      "V3               float64\n",
      "V4               float64\n",
      "V5               float64\n",
      "V6               float64\n",
      "V7               float64\n",
      "V8               float64\n",
      "V9               float64\n",
      "V10              float64\n",
      "V11              float64\n",
      "V12              float64\n",
      "V13              float64\n",
      "V14              float64\n",
      "V15              float64\n",
      "V16              float64\n",
      "V17              float64\n",
      "V18              float64\n",
      "V19              float64\n",
      "V20              float64\n",
      "V21              float64\n",
      "V22              float64\n",
      "V23              float64\n",
      "V24              float64\n",
      "V25              float64\n",
      "V26              float64\n",
      "V27              float64\n",
      "V28              float64\n",
      "V29              float64\n",
      "V30              float64\n",
      "                  ...   \n",
      "V191             float64\n",
      "V192             float64\n",
      "V193             float64\n",
      "V194             float64\n",
      "V195             float64\n",
      "V196             float64\n",
      "V197             float64\n",
      "V198             float64\n",
      "V199             float64\n",
      "V200             float64\n",
      "V201             float64\n",
      "V202             float64\n",
      "V203             float64\n",
      "V204             float64\n",
      "V205             float64\n",
      "V206             float64\n",
      "V207             float64\n",
      "V208             float64\n",
      "V209             float64\n",
      "V210             float64\n",
      "V211             float64\n",
      "V212             float64\n",
      "V213             float64\n",
      "V214             float64\n",
      "V215             float64\n",
      "V216             float64\n",
      "V217             float64\n",
      "V218             float64\n",
      "V219             float64\n",
      "Machine_State     object\n",
      "Length: 220, dtype: object\n",
      "Test datatypes  Sl No.      int64\n",
      "V1        float64\n",
      "V2        float64\n",
      "V3        float64\n",
      "V4        float64\n",
      "V5        float64\n",
      "V6        float64\n",
      "V7        float64\n",
      "V8        float64\n",
      "V9        float64\n",
      "V10       float64\n",
      "V11       float64\n",
      "V12       float64\n",
      "V13       float64\n",
      "V14       float64\n",
      "V15       float64\n",
      "V16       float64\n",
      "V17       float64\n",
      "V18       float64\n",
      "V19       float64\n",
      "V20       float64\n",
      "V21       float64\n",
      "V22       float64\n",
      "V23       float64\n",
      "V24       float64\n",
      "V25       float64\n",
      "V26       float64\n",
      "V27       float64\n",
      "V28       float64\n",
      "V29       float64\n",
      "           ...   \n",
      "V190      float64\n",
      "V191      float64\n",
      "V192      float64\n",
      "V193      float64\n",
      "V194      float64\n",
      "V195      float64\n",
      "V196      float64\n",
      "V197      float64\n",
      "V198      float64\n",
      "V199        int64\n",
      "V200      float64\n",
      "V201      float64\n",
      "V202      float64\n",
      "V203      float64\n",
      "V204      float64\n",
      "V205      float64\n",
      "V206      float64\n",
      "V207      float64\n",
      "V208      float64\n",
      "V209      float64\n",
      "V210      float64\n",
      "V211      float64\n",
      "V212      float64\n",
      "V213      float64\n",
      "V214      float64\n",
      "V215      float64\n",
      "V216      float64\n",
      "V217      float64\n",
      "V218      float64\n",
      "V219      float64\n",
      "Length: 220, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Train datatypes \" , Train_data.dtypes)\n",
    "print(\"Test datatypes \" , Test_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Target Variable datatype to factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_data.Machine_State = Train_data.Machine_State.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good    3240\n",
      "Bad      463\n",
      "Name: Machine_State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking whether Response variable data distribution is balanced or not\n",
    "print(pd.value_counts(Train_data['Machine_State']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Target variable data distribution for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAELCAYAAADOeWEXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6mJwSJK6/H89jrI0cHV2+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12143b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Machine_State', data =Train_data,palette= 'hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of Null values per column on training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1               56\n",
       "V8               56\n",
       "V15              56\n",
       "V22              56\n",
       "V29              56\n",
       "V36              56\n",
       "V43              56\n",
       "V50              56\n",
       "V57              56\n",
       "V64              56\n",
       "V71              56\n",
       "V78              56\n",
       "V85              56\n",
       "V92              56\n",
       "V99              56\n",
       "V106             56\n",
       "V113             56\n",
       "V120             56\n",
       "V127             56\n",
       "V134             56\n",
       "V141             56\n",
       "V148             56\n",
       "V155             56\n",
       "V162             56\n",
       "V169             56\n",
       "V176             56\n",
       "V183             56\n",
       "V190             56\n",
       "V197             56\n",
       "V205             56\n",
       "V213             56\n",
       "Machine_State    19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "null_columns=Train_data.columns[Train_data.isnull().any()]\n",
    "Train_data[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1      5\n",
       "V8      5\n",
       "V15     5\n",
       "V22     5\n",
       "V29     5\n",
       "V36     5\n",
       "V43     5\n",
       "V50     5\n",
       "V57     5\n",
       "V64     5\n",
       "V71     5\n",
       "V78     5\n",
       "V85     5\n",
       "V92     5\n",
       "V99     5\n",
       "V106    5\n",
       "V113    5\n",
       "V120    5\n",
       "V127    5\n",
       "V134    5\n",
       "V141    5\n",
       "V148    5\n",
       "V155    5\n",
       "V162    5\n",
       "V169    5\n",
       "V176    5\n",
       "V183    5\n",
       "V190    5\n",
       "V197    5\n",
       "V205    5\n",
       "V213    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sum of Null values per column on test data set\n",
    "null_columns=Test_data.columns[Test_data.isnull().any()]\n",
    "Test_data[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imputing Na with it's column mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_imputedSet=  Train_data.fillna(Train_data.mean())\n",
    "Test_imputedSet=  Test_data.fillna(Test_data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine_State    19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verifying if any null value exists for Train data\n",
    "null_columns=Train_imputedSet.columns[Train_imputedSet.isnull().any()]\n",
    "Train_imputedSet[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verifying if any null value exists for Test data\n",
    "null_columns=Test_imputedSet.columns[Test_imputedSet.isnull().any()]\n",
    "Test_imputedSet[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Test data dimension and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 220)\n",
      "Index(['Sl No.', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9',\n",
      "       ...\n",
      "       'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218',\n",
      "       'V219'],\n",
      "      dtype='object', length=220)\n"
     ]
    }
   ],
   "source": [
    "print(Test_imputedSet.shape)\n",
    "print(Test_imputedSet.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 219)\n"
     ]
    }
   ],
   "source": [
    "#Remove the irrelevant column from Testdataset\n",
    "X_test = Test_imputedSet.loc[:,Test_imputedSet.columns != 'Sl No.']\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "3    4\n",
       "4    5\n",
       "Name: Sl No., dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To concat with Submission file\n",
    "SerialNum = Test_imputedSet[\"Sl No.\"]\n",
    "SerialNum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace null values with 'Bad' as it is important and less too\n",
    "#Train_imputedSet['Machine_State'].fillna('Bad', inplace=True)\n",
    "\n",
    "#Drop NA target variable\n",
    "Train_imputedSet.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write the data to csv\n",
    "Train_imputedSet.to_csv(\"Imputed_DataSet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3703, 220)\n"
     ]
    }
   ],
   "source": [
    "print(Train_imputedSet.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating target variable from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       ...\n",
      "       'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218',\n",
      "       'V219'],\n",
      "      dtype='object', length=219)\n",
      "(3703, 219)\n",
      "(3703, 1)\n"
     ]
    }
   ],
   "source": [
    "X = Train_imputedSet.loc[:,Train_imputedSet.columns != 'Machine_State']\n",
    "Y = Train_imputedSet.loc[:,Train_imputedSet.columns == 'Machine_State']\n",
    "print(X.columns)  \n",
    "print(X.shape)  \n",
    "print(Y.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      V1        V2        V3        V4        V5        V6        V7  \\\n",
      "V1   NaN  0.164772  0.525073  0.149435  0.110333  0.164800  0.090617   \n",
      "V2   NaN       NaN  0.256672  0.972623  0.917249  1.000000  0.895930   \n",
      "V3   NaN       NaN       NaN  0.217200  0.126087  0.256713  0.083616   \n",
      "V4   NaN       NaN       NaN       NaN  0.969685  0.972630  0.914870   \n",
      "V5   NaN       NaN       NaN       NaN       NaN  0.917250  0.903655   \n",
      "V6   NaN       NaN       NaN       NaN       NaN       NaN  0.895913   \n",
      "V7   NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V8   NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V9   NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V10  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V11  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V12  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V13  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V14  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V15  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V16  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V17  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V18  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V19  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V20  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V21  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V22  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V23  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V24  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V25  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V26  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V27  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V28  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V29  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V30  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "...   ..       ...       ...       ...       ...       ...       ...   \n",
      "V190 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V191 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V192 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V193 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V194 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V195 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V196 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V197 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V198 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V199 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V200 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V201 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V202 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V203 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V204 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V205 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V206 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V207 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V208 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V209 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V210 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V211 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V212 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V213 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V214 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V215 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V216 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V217 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V218 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V219 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "            V8        V9       V10    ...         V210      V211      V212  \\\n",
      "V1    0.999732  0.165216  0.524744    ...     0.040112  0.084196  0.052363   \n",
      "V2    0.164710  0.999009  0.256499    ...     0.064770  0.000694  0.202903   \n",
      "V3    0.524783  0.259405  0.999943    ...     0.042598  0.067780  0.091373   \n",
      "V4    0.149451  0.969425  0.217074    ...     0.065800  0.001104  0.229585   \n",
      "V5    0.110447  0.913995  0.126012    ...     0.056630  0.005898  0.235516   \n",
      "V6    0.164738  0.999010  0.256540    ...     0.064759  0.000693  0.203009   \n",
      "V7    0.090636  0.897150  0.083494    ...     0.046679  0.006766  0.197513   \n",
      "V8         NaN  0.165146  0.524558    ...     0.039829  0.084934  0.052332   \n",
      "V9         NaN       NaN  0.259232    ...     0.064858  0.001710  0.202595   \n",
      "V10        NaN       NaN       NaN    ...     0.042476  0.067939  0.091548   \n",
      "V11        NaN       NaN       NaN    ...     0.066293  0.002348  0.229988   \n",
      "V12        NaN       NaN       NaN    ...     0.056482  0.009460  0.238679   \n",
      "V13        NaN       NaN       NaN    ...     0.064848  0.001709  0.202703   \n",
      "V14        NaN       NaN       NaN    ...     0.047127  0.004371  0.191895   \n",
      "V15        NaN       NaN       NaN    ...     0.042531  0.082960  0.053155   \n",
      "V16        NaN       NaN       NaN    ...     0.051606  0.026528  0.221251   \n",
      "V17        NaN       NaN       NaN    ...     0.042054  0.071945  0.101217   \n",
      "V18        NaN       NaN       NaN    ...     0.052232  0.038025  0.249166   \n",
      "V19        NaN       NaN       NaN    ...     0.037614  0.048256  0.252408   \n",
      "V20        NaN       NaN       NaN    ...     0.051594  0.026527  0.221378   \n",
      "V21        NaN       NaN       NaN    ...     0.033961  0.038076  0.199819   \n",
      "V22        NaN       NaN       NaN    ...     0.039550  0.093115  0.056290   \n",
      "V23        NaN       NaN       NaN    ...     0.053496  0.032668  0.222347   \n",
      "V24        NaN       NaN       NaN    ...     0.042654  0.070832  0.102028   \n",
      "V25        NaN       NaN       NaN    ...     0.054186  0.045554  0.251204   \n",
      "V26        NaN       NaN       NaN    ...     0.039846  0.059941  0.257617   \n",
      "V27        NaN       NaN       NaN    ...     0.053484  0.032666  0.222472   \n",
      "V28        NaN       NaN       NaN    ...     0.035282  0.045515  0.200197   \n",
      "V29        NaN       NaN       NaN    ...     0.019239  0.046070  0.074172   \n",
      "V30        NaN       NaN       NaN    ...     0.059140  0.037241  0.084751   \n",
      "...        ...       ...       ...    ...          ...       ...       ...   \n",
      "V190       NaN       NaN       NaN    ...     0.051337  0.174397  0.042392   \n",
      "V191       NaN       NaN       NaN    ...     0.060134  0.087338  0.123982   \n",
      "V192       NaN       NaN       NaN    ...     0.083656  0.131988  0.083829   \n",
      "V193       NaN       NaN       NaN    ...     0.067227  0.091653  0.152993   \n",
      "V194       NaN       NaN       NaN    ...     0.062044  0.090891  0.169007   \n",
      "V195       NaN       NaN       NaN    ...     0.060131  0.087346  0.124059   \n",
      "V196       NaN       NaN       NaN    ...     0.048150  0.072454  0.123000   \n",
      "V197       NaN       NaN       NaN    ...     0.019948  0.007017  0.000730   \n",
      "V198       NaN       NaN       NaN    ...     0.003758  0.011843  0.027009   \n",
      "V199       NaN       NaN       NaN    ...     0.007538  0.016679  0.002560   \n",
      "V200       NaN       NaN       NaN    ...     0.021584  0.007521  0.000792   \n",
      "V201       NaN       NaN       NaN    ...     0.023142  0.003122  0.005054   \n",
      "V202       NaN       NaN       NaN    ...     0.003756  0.011845  0.027012   \n",
      "V203       NaN       NaN       NaN    ...     0.023065  0.000857  0.000334   \n",
      "V204       NaN       NaN       NaN    ...     0.059110  0.079245  0.351837   \n",
      "V205       NaN       NaN       NaN    ...     0.950692  0.135782  0.041850   \n",
      "V206       NaN       NaN       NaN    ...     1.000000  0.098802  0.022957   \n",
      "V207       NaN       NaN       NaN    ...     0.967944  0.185086  0.090997   \n",
      "V208       NaN       NaN       NaN    ...     0.945325  0.081761  0.133637   \n",
      "V209       NaN       NaN       NaN    ...     0.014027  0.483422  0.427664   \n",
      "V210       NaN       NaN       NaN    ...          NaN  0.098807  0.022955   \n",
      "V211       NaN       NaN       NaN    ...          NaN       NaN  0.101817   \n",
      "V212       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V213       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V214       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V215       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V216       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V217       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V218       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V219       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "\n",
      "          V213      V214      V215      V216      V217      V218      V219  \n",
      "V1    0.076290  0.084725  0.012060  0.132257  0.169221  0.084727  0.021924  \n",
      "V2    0.011735  0.017402  0.098983  0.065286  0.199041  0.017410  0.003845  \n",
      "V3    0.048549  0.057725  0.037111  0.067928  0.052502  0.057724  0.067797  \n",
      "V4    0.015277  0.021072  0.082058  0.044102  0.151328  0.021077  0.002186  \n",
      "V5    0.003218  0.009069  0.059931  0.044117  0.124826  0.009079  0.001087  \n",
      "V6    0.011721  0.017387  0.098945  0.065274  0.198981  0.017395  0.003833  \n",
      "V7    0.002454  0.002262  0.073962  0.069266  0.173573  0.002278  0.011067  \n",
      "V8    0.076258  0.084708  0.012118  0.132172  0.169028  0.084710  0.021993  \n",
      "V9    0.011093  0.016819  0.101180  0.068218  0.205587  0.016828  0.003332  \n",
      "V10   0.048534  0.057725  0.037151  0.067911  0.052394  0.057725  0.067780  \n",
      "V11   0.014768  0.020831  0.084846  0.047186  0.158686  0.020836  0.001545  \n",
      "V12   0.001944  0.007828  0.062102  0.048278  0.133217  0.007839  0.000671  \n",
      "V13   0.011079  0.016804  0.101142  0.068205  0.205525  0.016813  0.003319  \n",
      "V14   0.003558  0.001055  0.075262  0.072432  0.179294  0.001072  0.010864  \n",
      "V15   0.074859  0.082587  0.008988  0.131336  0.171109  0.082589  0.019655  \n",
      "V16   0.002966  0.007486  0.109501  0.090846  0.251342  0.007496  0.010088  \n",
      "V17   0.047154  0.054293  0.033548  0.064447  0.051392  0.054291  0.061066  \n",
      "V18   0.004100  0.006037  0.089368  0.074783  0.205926  0.006041  0.016096  \n",
      "V19   0.009984  0.008496  0.062571  0.073996  0.173111  0.008487  0.014257  \n",
      "V20   0.002947  0.007469  0.109463  0.090836  0.251283  0.007478  0.010066  \n",
      "V21   0.013579  0.010804  0.082110  0.097600  0.227285  0.010788  0.020403  \n",
      "V22   0.076823  0.084749  0.009424  0.134584  0.175162  0.084750  0.020322  \n",
      "V23   0.001261  0.005093  0.108896  0.094650  0.256646  0.005106  0.013139  \n",
      "V24   0.047286  0.055804  0.033406  0.066763  0.055611  0.055803  0.057050  \n",
      "V25   0.001037  0.002169  0.088215  0.081951  0.214717  0.002175  0.019012  \n",
      "V26   0.012174  0.012390  0.061404  0.080930  0.181742  0.012380  0.018934  \n",
      "V27   0.001243  0.005077  0.108858  0.094640  0.256588  0.005089  0.013114  \n",
      "V28   0.018433  0.017061  0.078265  0.107037  0.237012  0.017044  0.023864  \n",
      "V29   0.048482  0.062133  0.062454  0.149180  0.247755  0.062081  0.068028  \n",
      "V30   0.080441  0.101149  0.029884  0.186483  0.259644  0.101096  0.071727  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "V190  0.123842  0.145890  0.002333  0.245435  0.331893  0.145890  0.027830  \n",
      "V191  0.115481  0.129556  0.086879  0.142686  0.105049  0.129572  0.002733  \n",
      "V192  0.127466  0.151325  0.058883  0.203505  0.208821  0.151323  0.026166  \n",
      "V193  0.123544  0.140777  0.079372  0.168199  0.147319  0.140792  0.012251  \n",
      "V194  0.112951  0.129694  0.057852  0.168698  0.173523  0.129713  0.014510  \n",
      "V195  0.115494  0.129566  0.086855  0.142728  0.105136  0.129582  0.002736  \n",
      "V196  0.088664  0.098263  0.059414  0.115572  0.101307  0.098285  0.002738  \n",
      "V197  0.030884  0.026630  0.022403  0.031933  0.016834  0.026629  0.005745  \n",
      "V198  0.012190  0.012754  0.001497  0.021374  0.026940  0.012753  0.000562  \n",
      "V199  0.007833  0.008792  0.001738  0.016612  0.024390  0.008792  0.000427  \n",
      "V200  0.032865  0.029150  0.023494  0.035112  0.019517  0.029146  0.006956  \n",
      "V201  0.029099  0.025845  0.021216  0.030943  0.016481  0.025841  0.006259  \n",
      "V202  0.012189  0.012752  0.001497  0.021371  0.026937  0.012751  0.000561  \n",
      "V203  0.030840  0.025808  0.026232  0.027471  0.006051  0.025805  0.007599  \n",
      "V204  0.213187  0.246518  0.014024  0.413651  0.550619  0.246531  0.047031  \n",
      "V205  0.523008  0.490811  0.496876  0.421907  0.009213  0.490815  0.041430  \n",
      "V206  0.513967  0.538320  0.526754  0.468478  0.037399  0.538321  0.062149  \n",
      "V207  0.508052  0.527187  0.540251  0.448597  0.009615  0.527187  0.070690  \n",
      "V208  0.484766  0.502587  0.480934  0.456234  0.081889  0.502587  0.054473  \n",
      "V209  0.007834  0.009989  0.077752  0.048973  0.205056  0.009988  0.020652  \n",
      "V210  0.514004  0.538356  0.526788  0.468512  0.037408  0.538357  0.062162  \n",
      "V211  0.008710  0.125367  0.070923  0.150563  0.135230  0.125367  0.450467  \n",
      "V212  0.022355  0.030957  0.014505  0.068896  0.107369  0.030947  0.022132  \n",
      "V213       NaN  0.947699  0.875995  0.885793  0.208010  0.947705  0.080118  \n",
      "V214       NaN       NaN  0.908723  0.932182  0.235421  1.000000  0.133787  \n",
      "V215       NaN       NaN       NaN  0.719139  0.163463  0.908729  0.140784  \n",
      "V216       NaN       NaN       NaN       NaN  0.551833  0.932178  0.115358  \n",
      "V217       NaN       NaN       NaN       NaN       NaN  0.235413  0.027440  \n",
      "V218       NaN       NaN       NaN       NaN       NaN       NaN  0.133795  \n",
      "V219       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "\n",
      "[219 rows x 219 columns]\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix ( Nan represents highly correlation between the variables)\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "print(upper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n",
      "['V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V30', 'V31', 'V32', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V65', 'V67', 'V68', 'V69', 'V70', 'V73', 'V74', 'V76', 'V79', 'V80', 'V81', 'V83', 'V87', 'V88', 'V89', 'V90', 'V91', 'V96', 'V97', 'V101', 'V103', 'V104', 'V107', 'V110', 'V111', 'V115', 'V117', 'V118', 'V125', 'V128', 'V132', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V156', 'V157', 'V158', 'V160', 'V163', 'V164', 'V165', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V199', 'V200', 'V201', 'V202', 'V203', 'V205', 'V206', 'V207', 'V208', 'V210', 'V214', 'V215', 'V216', 'V218']\n"
     ]
    }
   ],
   "source": [
    "#EXtract the columns which are highly coorelated \n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "print(len(to_drop))\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop the highly correlated columns\n",
    "dropped_df = X.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3703, 61)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the dimensions after dropping\n",
    "dropped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V29</th>\n",
       "      <th>V33</th>\n",
       "      <th>V57</th>\n",
       "      <th>V64</th>\n",
       "      <th>V66</th>\n",
       "      <th>V71</th>\n",
       "      <th>V72</th>\n",
       "      <th>...</th>\n",
       "      <th>V168</th>\n",
       "      <th>V197</th>\n",
       "      <th>V198</th>\n",
       "      <th>V204</th>\n",
       "      <th>V209</th>\n",
       "      <th>V211</th>\n",
       "      <th>V212</th>\n",
       "      <th>V213</th>\n",
       "      <th>V217</th>\n",
       "      <th>V219</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164772</td>\n",
       "      <td>0.525073</td>\n",
       "      <td>0.078567</td>\n",
       "      <td>0.060589</td>\n",
       "      <td>0.021213</td>\n",
       "      <td>0.588236</td>\n",
       "      <td>0.243176</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.005494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035978</td>\n",
       "      <td>0.013728</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.026723</td>\n",
       "      <td>0.084196</td>\n",
       "      <td>0.052363</td>\n",
       "      <td>0.076290</td>\n",
       "      <td>0.169221</td>\n",
       "      <td>0.021924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>0.164772</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.256672</td>\n",
       "      <td>0.252714</td>\n",
       "      <td>0.714931</td>\n",
       "      <td>0.025463</td>\n",
       "      <td>0.162511</td>\n",
       "      <td>0.346018</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.141455</td>\n",
       "      <td>0.059831</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.202903</td>\n",
       "      <td>0.011735</td>\n",
       "      <td>0.199041</td>\n",
       "      <td>0.003845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>0.525073</td>\n",
       "      <td>0.256672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031332</td>\n",
       "      <td>0.111155</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.420424</td>\n",
       "      <td>0.532158</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032005</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.053341</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.067780</td>\n",
       "      <td>0.091373</td>\n",
       "      <td>0.048549</td>\n",
       "      <td>0.052502</td>\n",
       "      <td>0.067797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V29</th>\n",
       "      <td>0.078567</td>\n",
       "      <td>0.252714</td>\n",
       "      <td>0.031332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395709</td>\n",
       "      <td>0.267549</td>\n",
       "      <td>0.092134</td>\n",
       "      <td>0.060104</td>\n",
       "      <td>0.045258</td>\n",
       "      <td>0.048409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041013</td>\n",
       "      <td>0.020839</td>\n",
       "      <td>0.017295</td>\n",
       "      <td>0.047651</td>\n",
       "      <td>0.183742</td>\n",
       "      <td>0.046070</td>\n",
       "      <td>0.074172</td>\n",
       "      <td>0.048482</td>\n",
       "      <td>0.247755</td>\n",
       "      <td>0.068028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V33</th>\n",
       "      <td>0.060589</td>\n",
       "      <td>0.714931</td>\n",
       "      <td>0.111155</td>\n",
       "      <td>0.395709</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.254586</td>\n",
       "      <td>0.185031</td>\n",
       "      <td>0.439326</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036678</td>\n",
       "      <td>0.008921</td>\n",
       "      <td>0.032319</td>\n",
       "      <td>0.151183</td>\n",
       "      <td>0.096099</td>\n",
       "      <td>0.103397</td>\n",
       "      <td>0.033966</td>\n",
       "      <td>0.199822</td>\n",
       "      <td>0.034295</td>\n",
       "      <td>0.033442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V57</th>\n",
       "      <td>0.021213</td>\n",
       "      <td>0.025463</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.267549</td>\n",
       "      <td>0.254586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033783</td>\n",
       "      <td>0.036850</td>\n",
       "      <td>0.056106</td>\n",
       "      <td>0.057690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.097599</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.043464</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>0.053557</td>\n",
       "      <td>0.388307</td>\n",
       "      <td>0.018967</td>\n",
       "      <td>0.021013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V64</th>\n",
       "      <td>0.588236</td>\n",
       "      <td>0.162511</td>\n",
       "      <td>0.420424</td>\n",
       "      <td>0.092134</td>\n",
       "      <td>0.185031</td>\n",
       "      <td>0.033783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.633490</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.007736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045332</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.170276</td>\n",
       "      <td>0.176821</td>\n",
       "      <td>0.309185</td>\n",
       "      <td>0.066612</td>\n",
       "      <td>0.100480</td>\n",
       "      <td>0.273925</td>\n",
       "      <td>0.013924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V66</th>\n",
       "      <td>0.243176</td>\n",
       "      <td>0.346018</td>\n",
       "      <td>0.532158</td>\n",
       "      <td>0.060104</td>\n",
       "      <td>0.439326</td>\n",
       "      <td>0.036850</td>\n",
       "      <td>0.633490</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.027374</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>0.117034</td>\n",
       "      <td>0.112182</td>\n",
       "      <td>0.187271</td>\n",
       "      <td>0.086840</td>\n",
       "      <td>0.070597</td>\n",
       "      <td>0.068254</td>\n",
       "      <td>0.005531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V71</th>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.045258</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>0.056106</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101132</td>\n",
       "      <td>0.010749</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>0.020632</td>\n",
       "      <td>0.039980</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.031857</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.022176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V72</th>\n",
       "      <td>0.005494</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.048409</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.057690</td>\n",
       "      <td>0.007736</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>0.724406</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109543</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.045686</td>\n",
       "      <td>0.013939</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.003168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V75</th>\n",
       "      <td>0.023095</td>\n",
       "      <td>0.075923</td>\n",
       "      <td>0.042176</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.017992</td>\n",
       "      <td>0.039186</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.037279</td>\n",
       "      <td>0.058321</td>\n",
       "      <td>0.019759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064623</td>\n",
       "      <td>0.031676</td>\n",
       "      <td>0.011425</td>\n",
       "      <td>0.092412</td>\n",
       "      <td>0.068151</td>\n",
       "      <td>0.004465</td>\n",
       "      <td>0.072529</td>\n",
       "      <td>0.015356</td>\n",
       "      <td>0.073781</td>\n",
       "      <td>0.013252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V77</th>\n",
       "      <td>0.023637</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.021278</td>\n",
       "      <td>0.060634</td>\n",
       "      <td>0.018548</td>\n",
       "      <td>0.038750</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.577050</td>\n",
       "      <td>0.054681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293418</td>\n",
       "      <td>0.005820</td>\n",
       "      <td>0.007512</td>\n",
       "      <td>0.043695</td>\n",
       "      <td>0.031890</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>0.012540</td>\n",
       "      <td>0.029732</td>\n",
       "      <td>0.028865</td>\n",
       "      <td>0.038583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V78</th>\n",
       "      <td>0.020282</td>\n",
       "      <td>0.012507</td>\n",
       "      <td>0.013327</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.004248</td>\n",
       "      <td>0.019306</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>0.385538</td>\n",
       "      <td>0.252957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.025601</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.010073</td>\n",
       "      <td>0.016404</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.002567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V82</th>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.148171</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.318393</td>\n",
       "      <td>0.119526</td>\n",
       "      <td>0.061292</td>\n",
       "      <td>0.004502</td>\n",
       "      <td>0.079028</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.024441</td>\n",
       "      <td>0.013781</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.029772</td>\n",
       "      <td>0.025725</td>\n",
       "      <td>0.139273</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>0.113540</td>\n",
       "      <td>0.009942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V84</th>\n",
       "      <td>0.029294</td>\n",
       "      <td>0.013868</td>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.035791</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.023708</td>\n",
       "      <td>0.047744</td>\n",
       "      <td>0.024668</td>\n",
       "      <td>0.255923</td>\n",
       "      <td>0.069510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024003</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.003014</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.029089</td>\n",
       "      <td>0.013675</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.011632</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.028037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V85</th>\n",
       "      <td>0.016823</td>\n",
       "      <td>0.006358</td>\n",
       "      <td>0.013461</td>\n",
       "      <td>0.028996</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.083893</td>\n",
       "      <td>0.025851</td>\n",
       "      <td>0.025404</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.775048</td>\n",
       "      <td>0.354298</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.045251</td>\n",
       "      <td>0.010923</td>\n",
       "      <td>0.029905</td>\n",
       "      <td>0.034153</td>\n",
       "      <td>0.014977</td>\n",
       "      <td>0.006764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V86</th>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.019905</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.026897</td>\n",
       "      <td>0.029842</td>\n",
       "      <td>0.093244</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.014879</td>\n",
       "      <td>0.014608</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.461420</td>\n",
       "      <td>0.662646</td>\n",
       "      <td>0.024028</td>\n",
       "      <td>0.065741</td>\n",
       "      <td>0.015831</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>0.013557</td>\n",
       "      <td>0.027293</td>\n",
       "      <td>0.000906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V92</th>\n",
       "      <td>0.007852</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>0.016816</td>\n",
       "      <td>0.288594</td>\n",
       "      <td>0.050848</td>\n",
       "      <td>0.143639</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.116916</td>\n",
       "      <td>0.098512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>0.033678</td>\n",
       "      <td>0.050302</td>\n",
       "      <td>0.108412</td>\n",
       "      <td>0.073215</td>\n",
       "      <td>0.077663</td>\n",
       "      <td>0.054456</td>\n",
       "      <td>0.062071</td>\n",
       "      <td>0.018570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V93</th>\n",
       "      <td>0.018843</td>\n",
       "      <td>0.575762</td>\n",
       "      <td>0.072405</td>\n",
       "      <td>0.401571</td>\n",
       "      <td>0.693993</td>\n",
       "      <td>0.083188</td>\n",
       "      <td>0.134149</td>\n",
       "      <td>0.374343</td>\n",
       "      <td>0.067785</td>\n",
       "      <td>0.136332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036538</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.022605</td>\n",
       "      <td>0.122997</td>\n",
       "      <td>0.032451</td>\n",
       "      <td>0.064693</td>\n",
       "      <td>0.142165</td>\n",
       "      <td>0.029419</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.059523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V94</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.087225</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.301320</td>\n",
       "      <td>0.048457</td>\n",
       "      <td>0.067697</td>\n",
       "      <td>0.012975</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>0.018154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.011274</td>\n",
       "      <td>0.024640</td>\n",
       "      <td>0.083110</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>0.052883</td>\n",
       "      <td>0.115318</td>\n",
       "      <td>0.131930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V95</th>\n",
       "      <td>0.023250</td>\n",
       "      <td>0.528391</td>\n",
       "      <td>0.054079</td>\n",
       "      <td>0.059608</td>\n",
       "      <td>0.546288</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.103598</td>\n",
       "      <td>0.306761</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.058793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015530</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>0.072355</td>\n",
       "      <td>0.013633</td>\n",
       "      <td>0.020417</td>\n",
       "      <td>0.089509</td>\n",
       "      <td>0.052611</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>0.007597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V98</th>\n",
       "      <td>0.015292</td>\n",
       "      <td>0.399063</td>\n",
       "      <td>0.028466</td>\n",
       "      <td>0.187630</td>\n",
       "      <td>0.407637</td>\n",
       "      <td>0.035857</td>\n",
       "      <td>0.078927</td>\n",
       "      <td>0.208071</td>\n",
       "      <td>0.019319</td>\n",
       "      <td>0.033173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009764</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.069142</td>\n",
       "      <td>0.007192</td>\n",
       "      <td>0.019643</td>\n",
       "      <td>0.106789</td>\n",
       "      <td>0.019617</td>\n",
       "      <td>0.026385</td>\n",
       "      <td>0.002817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V99</th>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.022274</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.215299</td>\n",
       "      <td>0.027630</td>\n",
       "      <td>0.137521</td>\n",
       "      <td>0.009413</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.174382</td>\n",
       "      <td>0.145782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>0.024117</td>\n",
       "      <td>0.031120</td>\n",
       "      <td>0.043814</td>\n",
       "      <td>0.093463</td>\n",
       "      <td>0.040244</td>\n",
       "      <td>0.059480</td>\n",
       "      <td>0.041967</td>\n",
       "      <td>0.043075</td>\n",
       "      <td>0.009439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V100</th>\n",
       "      <td>0.036947</td>\n",
       "      <td>0.569314</td>\n",
       "      <td>0.076910</td>\n",
       "      <td>0.308130</td>\n",
       "      <td>0.624120</td>\n",
       "      <td>0.080993</td>\n",
       "      <td>0.115139</td>\n",
       "      <td>0.315364</td>\n",
       "      <td>0.058269</td>\n",
       "      <td>0.105753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046823</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.129573</td>\n",
       "      <td>0.026293</td>\n",
       "      <td>0.045778</td>\n",
       "      <td>0.193899</td>\n",
       "      <td>0.040689</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.065743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V102</th>\n",
       "      <td>0.031435</td>\n",
       "      <td>0.519671</td>\n",
       "      <td>0.051189</td>\n",
       "      <td>0.055283</td>\n",
       "      <td>0.474846</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>0.070052</td>\n",
       "      <td>0.235063</td>\n",
       "      <td>0.017144</td>\n",
       "      <td>0.051797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027563</td>\n",
       "      <td>0.012152</td>\n",
       "      <td>0.009723</td>\n",
       "      <td>0.116441</td>\n",
       "      <td>0.070824</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.182974</td>\n",
       "      <td>0.052991</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.007429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V105</th>\n",
       "      <td>0.007914</td>\n",
       "      <td>0.413909</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.231107</td>\n",
       "      <td>0.416898</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.020652</td>\n",
       "      <td>0.169926</td>\n",
       "      <td>0.033811</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076633</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.007346</td>\n",
       "      <td>0.113695</td>\n",
       "      <td>0.050977</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.183647</td>\n",
       "      <td>0.026750</td>\n",
       "      <td>0.011816</td>\n",
       "      <td>0.019131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V106</th>\n",
       "      <td>0.006670</td>\n",
       "      <td>0.011513</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.110009</td>\n",
       "      <td>0.040599</td>\n",
       "      <td>0.095905</td>\n",
       "      <td>0.009169</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.032458</td>\n",
       "      <td>0.025747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015560</td>\n",
       "      <td>0.021121</td>\n",
       "      <td>0.017707</td>\n",
       "      <td>0.036817</td>\n",
       "      <td>0.057050</td>\n",
       "      <td>0.020206</td>\n",
       "      <td>0.031203</td>\n",
       "      <td>0.019541</td>\n",
       "      <td>0.035629</td>\n",
       "      <td>0.015729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V108</th>\n",
       "      <td>0.004410</td>\n",
       "      <td>0.155660</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.271191</td>\n",
       "      <td>0.094728</td>\n",
       "      <td>0.076744</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>0.063870</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021889</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.024975</td>\n",
       "      <td>0.044948</td>\n",
       "      <td>0.070680</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>0.065618</td>\n",
       "      <td>0.101455</td>\n",
       "      <td>0.125820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V109</th>\n",
       "      <td>0.011598</td>\n",
       "      <td>0.503818</td>\n",
       "      <td>0.053433</td>\n",
       "      <td>0.092785</td>\n",
       "      <td>0.602260</td>\n",
       "      <td>0.021091</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.283858</td>\n",
       "      <td>0.028431</td>\n",
       "      <td>0.071750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019684</td>\n",
       "      <td>0.018089</td>\n",
       "      <td>0.020947</td>\n",
       "      <td>0.090694</td>\n",
       "      <td>0.032433</td>\n",
       "      <td>0.013345</td>\n",
       "      <td>0.051816</td>\n",
       "      <td>0.057092</td>\n",
       "      <td>0.032216</td>\n",
       "      <td>0.008461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V112</th>\n",
       "      <td>0.014384</td>\n",
       "      <td>0.438434</td>\n",
       "      <td>0.017430</td>\n",
       "      <td>0.285606</td>\n",
       "      <td>0.532213</td>\n",
       "      <td>0.013713</td>\n",
       "      <td>0.072018</td>\n",
       "      <td>0.208409</td>\n",
       "      <td>0.041159</td>\n",
       "      <td>0.039403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045597</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>0.100512</td>\n",
       "      <td>0.021476</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.101349</td>\n",
       "      <td>0.022670</td>\n",
       "      <td>0.013104</td>\n",
       "      <td>0.028031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V114</th>\n",
       "      <td>0.055756</td>\n",
       "      <td>0.578957</td>\n",
       "      <td>0.108840</td>\n",
       "      <td>0.355018</td>\n",
       "      <td>0.771736</td>\n",
       "      <td>0.097048</td>\n",
       "      <td>0.153839</td>\n",
       "      <td>0.355882</td>\n",
       "      <td>0.084732</td>\n",
       "      <td>0.139246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017420</td>\n",
       "      <td>0.041035</td>\n",
       "      <td>0.051812</td>\n",
       "      <td>0.091002</td>\n",
       "      <td>0.079052</td>\n",
       "      <td>0.077040</td>\n",
       "      <td>0.061979</td>\n",
       "      <td>0.013143</td>\n",
       "      <td>0.024999</td>\n",
       "      <td>0.083265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V116</th>\n",
       "      <td>0.021234</td>\n",
       "      <td>0.431458</td>\n",
       "      <td>0.054499</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.497771</td>\n",
       "      <td>0.014346</td>\n",
       "      <td>0.093128</td>\n",
       "      <td>0.253894</td>\n",
       "      <td>0.027339</td>\n",
       "      <td>0.081025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>0.020440</td>\n",
       "      <td>0.104428</td>\n",
       "      <td>0.022399</td>\n",
       "      <td>0.014718</td>\n",
       "      <td>0.074737</td>\n",
       "      <td>0.061194</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.025065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V119</th>\n",
       "      <td>0.009473</td>\n",
       "      <td>0.453095</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.290224</td>\n",
       "      <td>0.556250</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.070666</td>\n",
       "      <td>0.206058</td>\n",
       "      <td>0.049685</td>\n",
       "      <td>0.025991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059429</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>0.103174</td>\n",
       "      <td>0.023239</td>\n",
       "      <td>0.021707</td>\n",
       "      <td>0.087009</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>0.009981</td>\n",
       "      <td>0.051464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V120</th>\n",
       "      <td>0.024479</td>\n",
       "      <td>0.123279</td>\n",
       "      <td>0.035893</td>\n",
       "      <td>0.102624</td>\n",
       "      <td>0.218547</td>\n",
       "      <td>0.224462</td>\n",
       "      <td>0.117536</td>\n",
       "      <td>0.168481</td>\n",
       "      <td>0.042805</td>\n",
       "      <td>0.024601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.330656</td>\n",
       "      <td>0.288206</td>\n",
       "      <td>0.118536</td>\n",
       "      <td>0.034621</td>\n",
       "      <td>0.031875</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.158258</td>\n",
       "      <td>0.056918</td>\n",
       "      <td>0.004132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V121</th>\n",
       "      <td>0.043232</td>\n",
       "      <td>0.115313</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>0.030549</td>\n",
       "      <td>0.155389</td>\n",
       "      <td>0.109730</td>\n",
       "      <td>0.032174</td>\n",
       "      <td>0.069082</td>\n",
       "      <td>0.031589</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010394</td>\n",
       "      <td>0.238220</td>\n",
       "      <td>0.305404</td>\n",
       "      <td>0.215628</td>\n",
       "      <td>0.135390</td>\n",
       "      <td>0.107591</td>\n",
       "      <td>0.100124</td>\n",
       "      <td>0.150176</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>0.004020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V122</th>\n",
       "      <td>0.039066</td>\n",
       "      <td>0.034196</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.132005</td>\n",
       "      <td>0.061508</td>\n",
       "      <td>0.045776</td>\n",
       "      <td>0.183446</td>\n",
       "      <td>0.075541</td>\n",
       "      <td>0.012944</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022755</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.097070</td>\n",
       "      <td>0.064537</td>\n",
       "      <td>0.174386</td>\n",
       "      <td>0.250981</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.052128</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.001032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V123</th>\n",
       "      <td>0.037099</td>\n",
       "      <td>0.265170</td>\n",
       "      <td>0.072702</td>\n",
       "      <td>0.062577</td>\n",
       "      <td>0.304482</td>\n",
       "      <td>0.146078</td>\n",
       "      <td>0.179327</td>\n",
       "      <td>0.261780</td>\n",
       "      <td>0.042441</td>\n",
       "      <td>0.023787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019876</td>\n",
       "      <td>0.305268</td>\n",
       "      <td>0.290885</td>\n",
       "      <td>0.220502</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>0.056134</td>\n",
       "      <td>0.115467</td>\n",
       "      <td>0.200473</td>\n",
       "      <td>0.133672</td>\n",
       "      <td>0.009966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V124</th>\n",
       "      <td>0.048836</td>\n",
       "      <td>0.129674</td>\n",
       "      <td>0.041528</td>\n",
       "      <td>0.183441</td>\n",
       "      <td>0.136754</td>\n",
       "      <td>0.054171</td>\n",
       "      <td>0.273243</td>\n",
       "      <td>0.231945</td>\n",
       "      <td>0.019057</td>\n",
       "      <td>0.020687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038141</td>\n",
       "      <td>0.218596</td>\n",
       "      <td>0.170365</td>\n",
       "      <td>0.118602</td>\n",
       "      <td>0.120661</td>\n",
       "      <td>0.256952</td>\n",
       "      <td>0.068526</td>\n",
       "      <td>0.102666</td>\n",
       "      <td>0.106746</td>\n",
       "      <td>0.008345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V126</th>\n",
       "      <td>0.035973</td>\n",
       "      <td>0.028308</td>\n",
       "      <td>0.029669</td>\n",
       "      <td>0.190180</td>\n",
       "      <td>0.076719</td>\n",
       "      <td>0.158620</td>\n",
       "      <td>0.095549</td>\n",
       "      <td>0.113448</td>\n",
       "      <td>0.012374</td>\n",
       "      <td>0.015503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033694</td>\n",
       "      <td>0.280281</td>\n",
       "      <td>0.151684</td>\n",
       "      <td>0.081406</td>\n",
       "      <td>0.166708</td>\n",
       "      <td>0.067934</td>\n",
       "      <td>0.129529</td>\n",
       "      <td>0.006328</td>\n",
       "      <td>0.036173</td>\n",
       "      <td>0.022472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V127</th>\n",
       "      <td>0.014733</td>\n",
       "      <td>0.096063</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.083801</td>\n",
       "      <td>0.058743</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.028779</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.024965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053291</td>\n",
       "      <td>0.216594</td>\n",
       "      <td>0.282699</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.085018</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.177310</td>\n",
       "      <td>0.026371</td>\n",
       "      <td>0.098456</td>\n",
       "      <td>0.001871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V129</th>\n",
       "      <td>0.009710</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.031492</td>\n",
       "      <td>0.068995</td>\n",
       "      <td>0.038607</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.010713</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.012187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017581</td>\n",
       "      <td>0.010661</td>\n",
       "      <td>0.025816</td>\n",
       "      <td>0.156312</td>\n",
       "      <td>0.169290</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>0.341876</td>\n",
       "      <td>0.057988</td>\n",
       "      <td>0.166393</td>\n",
       "      <td>0.005595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V130</th>\n",
       "      <td>0.010954</td>\n",
       "      <td>0.120154</td>\n",
       "      <td>0.022328</td>\n",
       "      <td>0.115095</td>\n",
       "      <td>0.053549</td>\n",
       "      <td>0.033165</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.029130</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080794</td>\n",
       "      <td>0.233685</td>\n",
       "      <td>0.315459</td>\n",
       "      <td>0.037275</td>\n",
       "      <td>0.065589</td>\n",
       "      <td>0.013139</td>\n",
       "      <td>0.106835</td>\n",
       "      <td>0.020693</td>\n",
       "      <td>0.046001</td>\n",
       "      <td>0.003590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V131</th>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.039181</td>\n",
       "      <td>0.136264</td>\n",
       "      <td>0.009747</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.184182</td>\n",
       "      <td>0.256997</td>\n",
       "      <td>0.144764</td>\n",
       "      <td>0.171113</td>\n",
       "      <td>0.040702</td>\n",
       "      <td>0.326795</td>\n",
       "      <td>0.057241</td>\n",
       "      <td>0.158881</td>\n",
       "      <td>0.005693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V133</th>\n",
       "      <td>0.026090</td>\n",
       "      <td>0.027879</td>\n",
       "      <td>0.045566</td>\n",
       "      <td>0.069431</td>\n",
       "      <td>0.032190</td>\n",
       "      <td>0.070991</td>\n",
       "      <td>0.021402</td>\n",
       "      <td>0.028604</td>\n",
       "      <td>0.008978</td>\n",
       "      <td>0.006843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044967</td>\n",
       "      <td>0.243024</td>\n",
       "      <td>0.320577</td>\n",
       "      <td>0.086146</td>\n",
       "      <td>0.067764</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.057101</td>\n",
       "      <td>0.005146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V147</th>\n",
       "      <td>0.070106</td>\n",
       "      <td>0.083527</td>\n",
       "      <td>0.066623</td>\n",
       "      <td>0.130908</td>\n",
       "      <td>0.143311</td>\n",
       "      <td>0.186524</td>\n",
       "      <td>0.195782</td>\n",
       "      <td>0.188203</td>\n",
       "      <td>0.033622</td>\n",
       "      <td>0.028286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018829</td>\n",
       "      <td>0.179253</td>\n",
       "      <td>0.082228</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.158263</td>\n",
       "      <td>0.127429</td>\n",
       "      <td>0.055410</td>\n",
       "      <td>0.056720</td>\n",
       "      <td>0.030848</td>\n",
       "      <td>0.012437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V155</th>\n",
       "      <td>0.021129</td>\n",
       "      <td>0.049939</td>\n",
       "      <td>0.015498</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.206882</td>\n",
       "      <td>0.630224</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>0.047271</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.007084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>0.032169</td>\n",
       "      <td>0.009837</td>\n",
       "      <td>0.057353</td>\n",
       "      <td>0.084424</td>\n",
       "      <td>0.078694</td>\n",
       "      <td>0.042687</td>\n",
       "      <td>0.644003</td>\n",
       "      <td>0.037647</td>\n",
       "      <td>0.057698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V159</th>\n",
       "      <td>0.022037</td>\n",
       "      <td>0.276736</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.216717</td>\n",
       "      <td>0.252358</td>\n",
       "      <td>0.044692</td>\n",
       "      <td>0.037151</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.018703</td>\n",
       "      <td>0.013731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035530</td>\n",
       "      <td>0.014907</td>\n",
       "      <td>0.019547</td>\n",
       "      <td>0.102609</td>\n",
       "      <td>0.548855</td>\n",
       "      <td>0.152025</td>\n",
       "      <td>0.135569</td>\n",
       "      <td>0.023046</td>\n",
       "      <td>0.337749</td>\n",
       "      <td>0.021354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V161</th>\n",
       "      <td>0.083552</td>\n",
       "      <td>0.094999</td>\n",
       "      <td>0.073403</td>\n",
       "      <td>0.121504</td>\n",
       "      <td>0.163702</td>\n",
       "      <td>0.019040</td>\n",
       "      <td>0.124592</td>\n",
       "      <td>0.087058</td>\n",
       "      <td>0.018544</td>\n",
       "      <td>0.014227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.018420</td>\n",
       "      <td>0.080143</td>\n",
       "      <td>0.256243</td>\n",
       "      <td>0.720387</td>\n",
       "      <td>0.026036</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>0.136103</td>\n",
       "      <td>0.569964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V162</th>\n",
       "      <td>0.020145</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.011021</td>\n",
       "      <td>0.030070</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.032722</td>\n",
       "      <td>0.028217</td>\n",
       "      <td>0.023083</td>\n",
       "      <td>0.149970</td>\n",
       "      <td>0.070993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498194</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.006650</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>0.025088</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>0.042855</td>\n",
       "      <td>0.008563</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.000351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V166</th>\n",
       "      <td>0.047580</td>\n",
       "      <td>0.077935</td>\n",
       "      <td>0.063256</td>\n",
       "      <td>0.135493</td>\n",
       "      <td>0.040099</td>\n",
       "      <td>0.030996</td>\n",
       "      <td>0.070869</td>\n",
       "      <td>0.045699</td>\n",
       "      <td>0.017760</td>\n",
       "      <td>0.043459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318887</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>0.007959</td>\n",
       "      <td>0.181452</td>\n",
       "      <td>0.206607</td>\n",
       "      <td>0.033142</td>\n",
       "      <td>0.386450</td>\n",
       "      <td>0.020072</td>\n",
       "      <td>0.080707</td>\n",
       "      <td>0.006724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V168</th>\n",
       "      <td>0.035978</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>0.032005</td>\n",
       "      <td>0.041013</td>\n",
       "      <td>0.036678</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.045332</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.101132</td>\n",
       "      <td>0.109543</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.059950</td>\n",
       "      <td>0.049859</td>\n",
       "      <td>0.003888</td>\n",
       "      <td>0.101264</td>\n",
       "      <td>0.014114</td>\n",
       "      <td>0.036239</td>\n",
       "      <td>0.004878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V197</th>\n",
       "      <td>0.013728</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>0.020839</td>\n",
       "      <td>0.008921</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.027374</td>\n",
       "      <td>0.010749</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733499</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>0.029872</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>0.016834</td>\n",
       "      <td>0.005745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V198</th>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.017295</td>\n",
       "      <td>0.032319</td>\n",
       "      <td>0.097599</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.733499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.011843</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.012190</td>\n",
       "      <td>0.026940</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V204</th>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.141455</td>\n",
       "      <td>0.053341</td>\n",
       "      <td>0.047651</td>\n",
       "      <td>0.151183</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.170276</td>\n",
       "      <td>0.117034</td>\n",
       "      <td>0.020632</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059950</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.131108</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.351837</td>\n",
       "      <td>0.213187</td>\n",
       "      <td>0.550619</td>\n",
       "      <td>0.047031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V209</th>\n",
       "      <td>0.026723</td>\n",
       "      <td>0.059831</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.183742</td>\n",
       "      <td>0.096099</td>\n",
       "      <td>0.043464</td>\n",
       "      <td>0.176821</td>\n",
       "      <td>0.112182</td>\n",
       "      <td>0.039980</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049859</td>\n",
       "      <td>0.029872</td>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.131108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.483422</td>\n",
       "      <td>0.427664</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>0.205056</td>\n",
       "      <td>0.020652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V211</th>\n",
       "      <td>0.084196</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.067780</td>\n",
       "      <td>0.046070</td>\n",
       "      <td>0.103397</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>0.309185</td>\n",
       "      <td>0.187271</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003888</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.011843</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.483422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101817</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>0.135230</td>\n",
       "      <td>0.450467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V212</th>\n",
       "      <td>0.052363</td>\n",
       "      <td>0.202903</td>\n",
       "      <td>0.091373</td>\n",
       "      <td>0.074172</td>\n",
       "      <td>0.033966</td>\n",
       "      <td>0.053557</td>\n",
       "      <td>0.066612</td>\n",
       "      <td>0.086840</td>\n",
       "      <td>0.031857</td>\n",
       "      <td>0.045686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101264</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.351837</td>\n",
       "      <td>0.427664</td>\n",
       "      <td>0.101817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.107369</td>\n",
       "      <td>0.022132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V213</th>\n",
       "      <td>0.076290</td>\n",
       "      <td>0.011735</td>\n",
       "      <td>0.048549</td>\n",
       "      <td>0.048482</td>\n",
       "      <td>0.199822</td>\n",
       "      <td>0.388307</td>\n",
       "      <td>0.100480</td>\n",
       "      <td>0.070597</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.013939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014114</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>0.012190</td>\n",
       "      <td>0.213187</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208010</td>\n",
       "      <td>0.080118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V217</th>\n",
       "      <td>0.169221</td>\n",
       "      <td>0.199041</td>\n",
       "      <td>0.052502</td>\n",
       "      <td>0.247755</td>\n",
       "      <td>0.034295</td>\n",
       "      <td>0.018967</td>\n",
       "      <td>0.273925</td>\n",
       "      <td>0.068254</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036239</td>\n",
       "      <td>0.016834</td>\n",
       "      <td>0.026940</td>\n",
       "      <td>0.550619</td>\n",
       "      <td>0.205056</td>\n",
       "      <td>0.135230</td>\n",
       "      <td>0.107369</td>\n",
       "      <td>0.208010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V219</th>\n",
       "      <td>0.021924</td>\n",
       "      <td>0.003845</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.068028</td>\n",
       "      <td>0.033442</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>0.022176</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.047031</td>\n",
       "      <td>0.020652</td>\n",
       "      <td>0.450467</td>\n",
       "      <td>0.022132</td>\n",
       "      <td>0.080118</td>\n",
       "      <td>0.027440</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows  61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            V1        V2        V3       V29       V33       V57       V64  \\\n",
       "V1    1.000000  0.164772  0.525073  0.078567  0.060589  0.021213  0.588236   \n",
       "V2    0.164772  1.000000  0.256672  0.252714  0.714931  0.025463  0.162511   \n",
       "V3    0.525073  0.256672  1.000000  0.031332  0.111155  0.001457  0.420424   \n",
       "V29   0.078567  0.252714  0.031332  1.000000  0.395709  0.267549  0.092134   \n",
       "V33   0.060589  0.714931  0.111155  0.395709  1.000000  0.254586  0.185031   \n",
       "V57   0.021213  0.025463  0.001457  0.267549  0.254586  1.000000  0.033783   \n",
       "V64   0.588236  0.162511  0.420424  0.092134  0.185031  0.033783  1.000000   \n",
       "V66   0.243176  0.346018  0.532158  0.060104  0.439326  0.036850  0.633490   \n",
       "V71   0.005165  0.022192  0.004056  0.045258  0.018390  0.056106  0.016414   \n",
       "V72   0.005494  0.015374  0.003998  0.048409  0.004348  0.057690  0.007736   \n",
       "V75   0.023095  0.075923  0.042176  0.009927  0.017992  0.039186  0.012021   \n",
       "V77   0.023637  0.001137  0.018817  0.021278  0.060634  0.018548  0.038750   \n",
       "V78   0.020282  0.012507  0.013327  0.031384  0.003521  0.004248  0.019306   \n",
       "V82   0.004264  0.148171  0.036439  0.318393  0.119526  0.061292  0.004502   \n",
       "V84   0.029294  0.013868  0.023310  0.035791  0.036242  0.023708  0.047744   \n",
       "V85   0.016823  0.006358  0.013461  0.028996  0.000205  0.083893  0.025851   \n",
       "V86   0.000472  0.019905  0.003832  0.026897  0.029842  0.093244  0.000625   \n",
       "V92   0.007852  0.009991  0.016816  0.288594  0.050848  0.143639  0.000088   \n",
       "V93   0.018843  0.575762  0.072405  0.401571  0.693993  0.083188  0.134149   \n",
       "V94   0.000438  0.087225  0.001331  0.301320  0.048457  0.067697  0.012975   \n",
       "V95   0.023250  0.528391  0.054079  0.059608  0.546288  0.019230  0.103598   \n",
       "V98   0.015292  0.399063  0.028466  0.187630  0.407637  0.035857  0.078927   \n",
       "V99   0.005188  0.022274  0.000238  0.215299  0.027630  0.137521  0.009413   \n",
       "V100  0.036947  0.569314  0.076910  0.308130  0.624120  0.080993  0.115139   \n",
       "V102  0.031435  0.519671  0.051189  0.055283  0.474846  0.029650  0.070052   \n",
       "V105  0.007914  0.413909  0.000394  0.231107  0.416898  0.044401  0.020652   \n",
       "V106  0.006670  0.011513  0.006070  0.110009  0.040599  0.095905  0.009169   \n",
       "V108  0.004410  0.155660  0.005147  0.271191  0.094728  0.076744  0.009786   \n",
       "V109  0.011598  0.503818  0.053433  0.092785  0.602260  0.021091  0.102038   \n",
       "V112  0.014384  0.438434  0.017430  0.285606  0.532213  0.013713  0.072018   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "V114  0.055756  0.578957  0.108840  0.355018  0.771736  0.097048  0.153839   \n",
       "V116  0.021234  0.431458  0.054499  0.013889  0.497771  0.014346  0.093128   \n",
       "V119  0.009473  0.453095  0.009112  0.290224  0.556250  0.020977  0.070666   \n",
       "V120  0.024479  0.123279  0.035893  0.102624  0.218547  0.224462  0.117536   \n",
       "V121  0.043232  0.115313  0.016108  0.030549  0.155389  0.109730  0.032174   \n",
       "V122  0.039066  0.034196  0.001038  0.132005  0.061508  0.045776  0.183446   \n",
       "V123  0.037099  0.265170  0.072702  0.062577  0.304482  0.146078  0.179327   \n",
       "V124  0.048836  0.129674  0.041528  0.183441  0.136754  0.054171  0.273243   \n",
       "V126  0.035973  0.028308  0.029669  0.190180  0.076719  0.158620  0.095549   \n",
       "V127  0.014733  0.096063  0.000640  0.023733  0.083801  0.058743  0.002637   \n",
       "V129  0.009710  0.012062  0.031492  0.068995  0.038607  0.003859  0.001098   \n",
       "V130  0.010954  0.120154  0.022328  0.115095  0.053549  0.033165  0.005709   \n",
       "V131  0.010763  0.064362  0.039181  0.136264  0.009747  0.008657  0.001686   \n",
       "V133  0.026090  0.027879  0.045566  0.069431  0.032190  0.070991  0.021402   \n",
       "V147  0.070106  0.083527  0.066623  0.130908  0.143311  0.186524  0.195782   \n",
       "V155  0.021129  0.049939  0.015498  0.006736  0.206882  0.630224  0.018722   \n",
       "V159  0.022037  0.276736  0.013420  0.216717  0.252358  0.044692  0.037151   \n",
       "V161  0.083552  0.094999  0.073403  0.121504  0.163702  0.019040  0.124592   \n",
       "V162  0.020145  0.001650  0.011021  0.030070  0.001698  0.032722  0.028217   \n",
       "V166  0.047580  0.077935  0.063256  0.135493  0.040099  0.030996  0.070869   \n",
       "V168  0.035978  0.003934  0.032005  0.041013  0.036678  0.039835  0.045332   \n",
       "V197  0.013728  0.002111  0.007239  0.020839  0.008921  0.098333  0.018541   \n",
       "V198  0.002457  0.009535  0.000467  0.017295  0.032319  0.097599  0.000031   \n",
       "V204  0.014086  0.141455  0.053341  0.047651  0.151183  0.008322  0.170276   \n",
       "V209  0.026723  0.059831  0.004800  0.183742  0.096099  0.043464  0.176821   \n",
       "V211  0.084196  0.000694  0.067780  0.046070  0.103397  0.012034  0.309185   \n",
       "V212  0.052363  0.202903  0.091373  0.074172  0.033966  0.053557  0.066612   \n",
       "V213  0.076290  0.011735  0.048549  0.048482  0.199822  0.388307  0.100480   \n",
       "V217  0.169221  0.199041  0.052502  0.247755  0.034295  0.018967  0.273925   \n",
       "V219  0.021924  0.003845  0.067797  0.068028  0.033442  0.021013  0.013924   \n",
       "\n",
       "           V66       V71       V72    ...         V168      V197      V198  \\\n",
       "V1    0.243176  0.005165  0.005494    ...     0.035978  0.013728  0.002457   \n",
       "V2    0.346018  0.022192  0.015374    ...     0.003934  0.002111  0.009535   \n",
       "V3    0.532158  0.004056  0.003998    ...     0.032005  0.007239  0.000467   \n",
       "V29   0.060104  0.045258  0.048409    ...     0.041013  0.020839  0.017295   \n",
       "V33   0.439326  0.018390  0.004348    ...     0.036678  0.008921  0.032319   \n",
       "V57   0.036850  0.056106  0.057690    ...     0.039835  0.098333  0.097599   \n",
       "V64   0.633490  0.016414  0.007736    ...     0.045332  0.018541  0.000031   \n",
       "V66   1.000000  0.000215  0.006867    ...     0.002210  0.027374  0.019202   \n",
       "V71   0.000215  1.000000  0.724406    ...     0.101132  0.010749  0.010854   \n",
       "V72   0.006867  0.724406  1.000000    ...     0.109543  0.005160  0.003558   \n",
       "V75   0.037279  0.058321  0.019759    ...     0.064623  0.031676  0.011425   \n",
       "V77   0.019779  0.577050  0.054681    ...     0.293418  0.005820  0.007512   \n",
       "V78   0.005603  0.385538  0.252957    ...     0.005174  0.002044  0.001966   \n",
       "V82   0.079028  0.002469  0.000476    ...     0.022068  0.024441  0.013781   \n",
       "V84   0.024668  0.255923  0.069510    ...     0.024003  0.003580  0.003014   \n",
       "V85   0.025404  0.010634  0.001123    ...     0.001877  0.775048  0.354298   \n",
       "V86   0.014879  0.014608  0.008399    ...     0.001433  0.461420  0.662646   \n",
       "V92   0.001943  0.116916  0.098512    ...     0.020332  0.025295  0.033678   \n",
       "V93   0.374343  0.067785  0.136332    ...     0.036538  0.007442  0.022605   \n",
       "V94   0.037319  0.020918  0.018154    ...     0.018166  0.004200  0.004114   \n",
       "V95   0.306761  0.015665  0.058793    ...     0.015530  0.010786  0.011081   \n",
       "V98   0.208071  0.019319  0.033173    ...     0.009764  0.000789  0.002552   \n",
       "V99   0.027522  0.174382  0.145782    ...     0.006211  0.024117  0.031120   \n",
       "V100  0.315364  0.058269  0.105753    ...     0.046823  0.009927  0.018696   \n",
       "V102  0.235063  0.017144  0.051797    ...     0.027563  0.012152  0.009723   \n",
       "V105  0.169926  0.033811  0.015883    ...     0.076633  0.001811  0.007346   \n",
       "V106  0.000550  0.032458  0.025747    ...     0.015560  0.021121  0.017707   \n",
       "V108  0.063870  0.003788  0.004983    ...     0.021889  0.005116  0.009960   \n",
       "V109  0.283858  0.028431  0.071750    ...     0.019684  0.018089  0.020947   \n",
       "V112  0.208409  0.041159  0.039403    ...     0.045597  0.000796  0.012229   \n",
       "...        ...       ...       ...    ...          ...       ...       ...   \n",
       "V114  0.355882  0.084732  0.139246    ...     0.017420  0.041035  0.051812   \n",
       "V116  0.253894  0.027339  0.081025    ...     0.014763  0.019454  0.020440   \n",
       "V119  0.206058  0.049685  0.025991    ...     0.059429  0.002940  0.009730   \n",
       "V120  0.168481  0.042805  0.024601    ...     0.000904  0.330656  0.288206   \n",
       "V121  0.069082  0.031589  0.004237    ...     0.010394  0.238220  0.305404   \n",
       "V122  0.075541  0.012944  0.004840    ...     0.022755  0.059850  0.097070   \n",
       "V123  0.261780  0.042441  0.023787    ...     0.019876  0.305268  0.290885   \n",
       "V124  0.231945  0.019057  0.020687    ...     0.038141  0.218596  0.170365   \n",
       "V126  0.113448  0.012374  0.015503    ...     0.033694  0.280281  0.151684   \n",
       "V127  0.028779  0.002872  0.024965    ...     0.053291  0.216594  0.282699   \n",
       "V129  0.010713  0.008017  0.012187    ...     0.017581  0.010661  0.025816   \n",
       "V130  0.029130  0.018115  0.002311    ...     0.080794  0.233685  0.315459   \n",
       "V131  0.003274  0.015827  0.010453    ...     0.042481  0.184182  0.256997   \n",
       "V133  0.028604  0.008978  0.006843    ...     0.044967  0.243024  0.320577   \n",
       "V147  0.188203  0.033622  0.028286    ...     0.018829  0.179253  0.082228   \n",
       "V155  0.047271  0.001323  0.007084    ...     0.005321  0.032169  0.009837   \n",
       "V159  0.043000  0.018703  0.013731    ...     0.035530  0.014907  0.019547   \n",
       "V161  0.087058  0.018544  0.014227    ...     0.011924  0.014874  0.018420   \n",
       "V162  0.023083  0.149970  0.070993    ...     0.498194  0.005259  0.006650   \n",
       "V166  0.045699  0.017760  0.043459    ...     0.318887  0.011250  0.007959   \n",
       "V168  0.002210  0.101132  0.109543    ...     1.000000  0.001329  0.000704   \n",
       "V197  0.027374  0.010749  0.005160    ...     0.001329  1.000000  0.733499   \n",
       "V198  0.019202  0.010854  0.003558    ...     0.000704  0.733499  1.000000   \n",
       "V204  0.117034  0.020632  0.000559    ...     0.059950  0.019474  0.032304   \n",
       "V209  0.112182  0.039980  0.010683    ...     0.049859  0.029872  0.028797   \n",
       "V211  0.187271  0.004999  0.000920    ...     0.003888  0.007017  0.011843   \n",
       "V212  0.086840  0.031857  0.045686    ...     0.101264  0.000730  0.027009   \n",
       "V213  0.070597  0.004954  0.013939    ...     0.014114  0.030884  0.012190   \n",
       "V217  0.068254  0.006837  0.001486    ...     0.036239  0.016834  0.026940   \n",
       "V219  0.005531  0.022176  0.003168    ...     0.004878  0.005745  0.000562   \n",
       "\n",
       "          V204      V209      V211      V212      V213      V217      V219  \n",
       "V1    0.014086  0.026723  0.084196  0.052363  0.076290  0.169221  0.021924  \n",
       "V2    0.141455  0.059831  0.000694  0.202903  0.011735  0.199041  0.003845  \n",
       "V3    0.053341  0.004800  0.067780  0.091373  0.048549  0.052502  0.067797  \n",
       "V29   0.047651  0.183742  0.046070  0.074172  0.048482  0.247755  0.068028  \n",
       "V33   0.151183  0.096099  0.103397  0.033966  0.199822  0.034295  0.033442  \n",
       "V57   0.008322  0.043464  0.012034  0.053557  0.388307  0.018967  0.021013  \n",
       "V64   0.170276  0.176821  0.309185  0.066612  0.100480  0.273925  0.013924  \n",
       "V66   0.117034  0.112182  0.187271  0.086840  0.070597  0.068254  0.005531  \n",
       "V71   0.020632  0.039980  0.004999  0.031857  0.004954  0.006837  0.022176  \n",
       "V72   0.000559  0.010683  0.000920  0.045686  0.013939  0.001486  0.003168  \n",
       "V75   0.092412  0.068151  0.004465  0.072529  0.015356  0.073781  0.013252  \n",
       "V77   0.043695  0.031890  0.006947  0.012540  0.029732  0.028865  0.038583  \n",
       "V78   0.025601  0.015200  0.010073  0.016404  0.002894  0.005441  0.002567  \n",
       "V82   0.002306  0.029772  0.025725  0.139273  0.011638  0.113540  0.009942  \n",
       "V84   0.006329  0.029089  0.013675  0.008222  0.011632  0.005266  0.028037  \n",
       "V85   0.005134  0.045251  0.010923  0.029905  0.034153  0.014977  0.006764  \n",
       "V86   0.024028  0.065741  0.015831  0.004793  0.013557  0.027293  0.000906  \n",
       "V92   0.050302  0.108412  0.073215  0.077663  0.054456  0.062071  0.018570  \n",
       "V93   0.122997  0.032451  0.064693  0.142165  0.029419  0.000851  0.059523  \n",
       "V94   0.011274  0.024640  0.083110  0.008746  0.052883  0.115318  0.131930  \n",
       "V95   0.072355  0.013633  0.020417  0.089509  0.052611  0.007191  0.007597  \n",
       "V98   0.069142  0.007192  0.019643  0.106789  0.019617  0.026385  0.002817  \n",
       "V99   0.043814  0.093463  0.040244  0.059480  0.041967  0.043075  0.009439  \n",
       "V100  0.129573  0.026293  0.045778  0.193899  0.040689  0.005088  0.065743  \n",
       "V102  0.116441  0.070824  0.001930  0.182974  0.052991  0.019441  0.007429  \n",
       "V105  0.113695  0.050977  0.000148  0.183647  0.026750  0.011816  0.019131  \n",
       "V106  0.036817  0.057050  0.020206  0.031203  0.019541  0.035629  0.015729  \n",
       "V108  0.024975  0.044948  0.070680  0.020478  0.065618  0.101455  0.125820  \n",
       "V109  0.090694  0.032433  0.013345  0.051816  0.057092  0.032216  0.008461  \n",
       "V112  0.100512  0.021476  0.011796  0.101349  0.022670  0.013104  0.028031  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "V114  0.091002  0.079052  0.077040  0.061979  0.013143  0.024999  0.083265  \n",
       "V116  0.104428  0.022399  0.014718  0.074737  0.061194  0.005083  0.025065  \n",
       "V119  0.103174  0.023239  0.021707  0.087009  0.033779  0.009981  0.051464  \n",
       "V120  0.118536  0.034621  0.031875  0.000909  0.158258  0.056918  0.004132  \n",
       "V121  0.215628  0.135390  0.107591  0.100124  0.150176  0.062184  0.004020  \n",
       "V122  0.064537  0.174386  0.250981  0.024876  0.052128  0.008842  0.001032  \n",
       "V123  0.220502  0.024632  0.056134  0.115467  0.200473  0.133672  0.009966  \n",
       "V124  0.118602  0.120661  0.256952  0.068526  0.102666  0.106746  0.008345  \n",
       "V126  0.081406  0.166708  0.067934  0.129529  0.006328  0.036173  0.022472  \n",
       "V127  0.096500  0.085018  0.017383  0.177310  0.026371  0.098456  0.001871  \n",
       "V129  0.156312  0.169290  0.041893  0.341876  0.057988  0.166393  0.005595  \n",
       "V130  0.037275  0.065589  0.013139  0.106835  0.020693  0.046001  0.003590  \n",
       "V131  0.144764  0.171113  0.040702  0.326795  0.057241  0.158881  0.005693  \n",
       "V133  0.086146  0.067764  0.003813  0.126500  0.014050  0.057101  0.005146  \n",
       "V147  0.007308  0.158263  0.127429  0.055410  0.056720  0.030848  0.012437  \n",
       "V155  0.057353  0.084424  0.078694  0.042687  0.644003  0.037647  0.057698  \n",
       "V159  0.102609  0.548855  0.152025  0.135569  0.023046  0.337749  0.021354  \n",
       "V161  0.080143  0.256243  0.720387  0.026036  0.009147  0.136103  0.569964  \n",
       "V162  0.024283  0.025088  0.004719  0.042855  0.008563  0.003969  0.000351  \n",
       "V166  0.181452  0.206607  0.033142  0.386450  0.020072  0.080707  0.006724  \n",
       "V168  0.059950  0.049859  0.003888  0.101264  0.014114  0.036239  0.004878  \n",
       "V197  0.019474  0.029872  0.007017  0.000730  0.030884  0.016834  0.005745  \n",
       "V198  0.032304  0.028797  0.011843  0.027009  0.012190  0.026940  0.000562  \n",
       "V204  1.000000  0.131108  0.079245  0.351837  0.213187  0.550619  0.047031  \n",
       "V209  0.131108  1.000000  0.483422  0.427664  0.007834  0.205056  0.020652  \n",
       "V211  0.079245  0.483422  1.000000  0.101817  0.008710  0.135230  0.450467  \n",
       "V212  0.351837  0.427664  0.101817  1.000000  0.022355  0.107369  0.022132  \n",
       "V213  0.213187  0.007834  0.008710  0.022355  1.000000  0.208010  0.080118  \n",
       "V217  0.550619  0.205056  0.135230  0.107369  0.208010  1.000000  0.027440  \n",
       "V219  0.047031  0.020652  0.450467  0.022132  0.080118  0.027440  1.000000  \n",
       "\n",
       "[61 rows x 61 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifying again if any correlation exist\n",
    "\n",
    "dropped_df.corr().abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping the same highly correlated variable from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['V1', 'V2', 'V3', 'V29', 'V33', 'V57', 'V64', 'V66', 'V71', 'V72',\n",
      "       'V75', 'V77', 'V78', 'V82', 'V84', 'V85', 'V86', 'V92', 'V93', 'V94',\n",
      "       'V95', 'V98', 'V99', 'V100', 'V102', 'V105', 'V106', 'V108', 'V109',\n",
      "       'V112', 'V113', 'V114', 'V116', 'V119', 'V120', 'V121', 'V122', 'V123',\n",
      "       'V124', 'V126', 'V127', 'V129', 'V130', 'V131', 'V133', 'V147', 'V155',\n",
      "       'V159', 'V161', 'V162', 'V166', 'V168', 'V197', 'V198', 'V204', 'V209',\n",
      "       'V211', 'V212', 'V213', 'V217', 'V219'],\n",
      "      dtype='object')\n",
      "(400, 61)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.columns)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling the data to balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(926, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "ros = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_sample(dropped_df,Y)\n",
    "print(X_resampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standardizing numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_validation = sc.transform(X_validation) \n",
    "X_test = sc.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the data distribution of train and validation target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train count ****  Counter({'Bad': 375, 'Good': 365})\n",
      "validation count ****  Counter({'Good': 98, 'Bad': 88})\n"
     ]
    }
   ],
   "source": [
    "train_count = collections.Counter(y_train)\n",
    "validation_count = collections.Counter(y_validation)\n",
    "print(\"train count **** \",train_count )\n",
    "print(\"validation count **** \",validation_count )\n",
    "#collections.Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to print TP,TN,FP,FN and calculate the total cost bear by manufacturing company on misclassifyng the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Positive is Bad , Negative is Good\n",
    "def print_confusion_matrix(true, pred):\n",
    "    cm = confusion_matrix(true, pred)\n",
    "    print('True positive = ', cm[0][0])\n",
    "    print('False positive = ', cm[1][0])\n",
    "    print('False negative = ', cm[0][1])\n",
    "    print('True negative = ', cm[1][1])\n",
    "    Total_Cost = (cost_FN * cm[0][1]) + (cost_FP * cm[1][0])\n",
    "    print(\"Total Cost \", Total_Cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 1.00\n",
      "[[335  40]\n",
      " [ 90 275]]\n",
      "True positive =  335\n",
      "False positive =  90\n",
      "False negative =  40\n",
      "True negative =  275\n",
      "Total Cost  245000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.79      0.89      0.84       375\n",
      "       Good       0.87      0.75      0.81       365\n",
      "\n",
      "avg / total       0.83      0.82      0.82       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Predicton and measure on training data for logistic model\n",
    "\n",
    "\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_train, y_train_pred)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.79\n",
      "[[77 11]\n",
      " [28 70]]\n",
      "True positive =  77\n",
      "False positive =  28\n",
      "False negative =  11\n",
      "True negative =  70\n",
      "Total Cost  69000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.73      0.88      0.80        88\n",
      "       Good       0.86      0.71      0.78        98\n",
      "\n",
      "avg / total       0.80      0.79      0.79       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on Validation data for logistic model\n",
    "\n",
    "y_pred = logreg.predict(X_validation)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_validation, y_validation)))\n",
    "#print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred)))\n",
    "#Confusion matrix\n",
    "\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "print(classification_report(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good    238\n",
      "Bad     162\n",
      "Name: Machine_State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#test predictions for logistic model\n",
    "\n",
    "test_logistic_pred = logreg.predict(X_test)\n",
    "pred_test_logistic = pd.DataFrame(test_logistic_pred, columns=['Machine_State'])\n",
    "print(pd.value_counts(pred_test_logistic['Machine_State']))\n",
    "\n",
    "#submission = pd.concat([SerialNum, pred_test_logistic], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search(Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 2.7825594022071245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Bad', 'Good',\n",
       "       'Bad', 'Good', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good', 'Bad', 'Good', 'Bad',\n",
       "       'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Good', 'Bad',\n",
       "       'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Good', 'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Good', 'Bad', 'Bad', 'Good',\n",
       "       'Bad', 'Good', 'Good', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Bad',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad',\n",
       "       'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Good', 'Good', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good',\n",
       "       'Bad', 'Good', 'Good', 'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Good',\n",
       "       'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Bad',\n",
       "       'Good', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Good',\n",
       "       'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Bad',\n",
       "       'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Bad',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good', 'Bad', 'Bad',\n",
       "       'Bad', 'Good', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Good', 'Bad',\n",
       "       'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Good', 'Bad', 'Good', 'Bad',\n",
       "       'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad',\n",
       "       'Bad', 'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Good',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Bad',\n",
       "       'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Good', 'Good', 'Good', 'Bad', 'Good', 'Bad', 'Bad',\n",
       "       'Good', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Bad',\n",
       "       'Bad', 'Good', 'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Bad',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Bad', 'Good', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Bad',\n",
       "       'Good', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Good',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Bad',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Good', 'Good', 'Good', 'Bad',\n",
       "       'Bad', 'Good', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Good',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Good', 'Good', 'Bad', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Bad',\n",
       "       'Good', 'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good',\n",
       "       'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good', 'Bad',\n",
       "       'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Good', 'Good', 'Bad', 'Bad', 'Bad',\n",
       "       'Good', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Bad',\n",
       "       'Good', 'Good', 'Bad', 'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Good',\n",
       "       'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Good', 'Bad',\n",
       "       'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Good',\n",
       "       'Bad', 'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Good',\n",
       "       'Good', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Good',\n",
       "       'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Good'], dtype=object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train,y_train)\n",
    "\n",
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "\n",
    "# Predict target vector\n",
    "best_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bad', 'Good', 'Good', 'Good', 'Bad', 'Good', 'Bad', 'Good',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good', 'Bad',\n",
       "       'Good', 'Bad', 'Good', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Bad',\n",
       "       'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Bad',\n",
       "       'Good', 'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Good',\n",
       "       'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Bad',\n",
       "       'Good', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Bad', 'Good',\n",
       "       'Good', 'Good', 'Good', 'Bad', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Bad',\n",
       "       'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Good', 'Bad', 'Good', 'Good', 'Good', 'Bad',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Good',\n",
       "       'Good', 'Bad', 'Good', 'Bad', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Bad', 'Good', 'Good', 'Bad', 'Good', 'Good', 'Bad', 'Good',\n",
       "       'Good', 'Bad', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Good',\n",
       "       'Good', 'Good', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Bad',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good', 'Good', 'Bad',\n",
       "       'Good', 'Bad', 'Good', 'Bad', 'Good', 'Good', 'Good', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Bad', 'Good',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Bad', 'Good', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Good', 'Good', 'Good', 'Good', 'Good', 'Bad',\n",
       "       'Bad', 'Good', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Good', 'Good', 'Good', 'Good', 'Good', 'Good', 'Good',\n",
       "       'Bad', 'Bad', 'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Bad',\n",
       "       'Good', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Bad', 'Good', 'Good',\n",
       "       'Good', 'Bad', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Good', 'Good', 'Good', 'Bad', 'Bad',\n",
       "       'Bad', 'Bad', 'Bad', 'Good', 'Bad', 'Good', 'Good', 'Bad', 'Good',\n",
       "       'Bad', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Good', 'Good', 'Good',\n",
       "       'Bad', 'Bad', 'Good', 'Good', 'Good', 'Bad'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_val = best_model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.74      0.84      0.79        88\n",
      "       Good       0.84      0.73      0.78        98\n",
      "\n",
      "avg / total       0.79      0.78      0.78       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_validation, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.68\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "naive = model.fit(X_train, y_train)\n",
    "naive_preds = naive.predict(X_validation)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, naive_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.64\n",
      "[[369   6]\n",
      " [257 108]]\n",
      "True positive =  369\n",
      "False positive =  257\n",
      "False negative =  6\n",
      "True negative =  108\n",
      "Total Cost  158500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.59      0.98      0.74       375\n",
      "       Good       0.95      0.30      0.45       365\n",
      "\n",
      "avg / total       0.77      0.64      0.60       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for Naive Bayes model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_naive = naive.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_naive)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_naive,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_naive)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.68\n",
      "[[87  1]\n",
      " [58 40]]\n",
      "True positive =  87\n",
      "False positive =  58\n",
      "False negative =  1\n",
      "True negative =  40\n",
      "Total Cost  34000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.60      0.99      0.75        88\n",
      "       Good       0.98      0.41      0.58        98\n",
      "\n",
      "avg / total       0.80      0.68      0.66       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for Naive Bayes model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation = naive.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad     293\n",
      "Good    107\n",
      "Name: Machine_State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#test predictions on naive Bayes model\n",
    "\n",
    "test_naive_pred = naive.predict(X_test)\n",
    "pred_test_naive = pd.DataFrame(test_naive_pred, columns=['Machine_State'])\n",
    "print(pd.value_counts(pred_test_naive['Machine_State']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, criterion='gini', min_samples_split=2)\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1.00\n",
      "[[375   0]\n",
      " [  0 365]]\n",
      "True positive =  375\n",
      "False positive =  0\n",
      "False negative =  0\n",
      "True negative =  365\n",
      "Total Cost  0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       1.00      1.00      1.00       375\n",
      "       Good       1.00      1.00      1.00       365\n",
      "\n",
      "avg / total       1.00      1.00      1.00       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for Random forest model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_rf)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_rf,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_rf)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.88\n",
      "[[84  4]\n",
      " [18 80]]\n",
      "True positive =  84\n",
      "False positive =  18\n",
      "False negative =  4\n",
      "True negative =  80\n",
      "Total Cost  29000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.82      0.95      0.88        88\n",
      "       Good       0.95      0.82      0.88        98\n",
      "\n",
      "avg / total       0.89      0.88      0.88       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for Random forest model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_rf = rf_model.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_rf)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_rf,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_rf)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good    287\n",
      "Bad     113\n",
      "Name: Machine_State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#test predictions using RandomForest model\n",
    "\n",
    "test_rf_pred = rf.predict(X_test)\n",
    "pred_test_rf = pd.DataFrame(test_rf_pred, columns=['Machine_State'])\n",
    "print(pd.value_counts(pred_test_rf['Machine_State']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_rf_pred = pd.DataFrame(test_rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"F:\\\\Noodle.ai\\\\ManufacturingQuality\")\n",
    "test_rf_pred.to_csv(\"Pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search(RandomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 6, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = { \n",
    "           \"n_estimators\" : [100,50,70,150,250,500],\n",
    "           \"max_depth\" : [20,30,50,60,80, 90,],\n",
    "           \"min_samples_split\" : [6, 8, 10,20,50,40,30],\n",
    "            \"criterion\": [\"gini\", \"entropy\"]}\n",
    " \n",
    "CV_rf = GridSearchCV(estimator=rf, param_grid=param_grid)\n",
    "CV_rf.fit(X_train, y_train)\n",
    "print(CV_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=250, criterion='gini', min_samples_split=6)\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1.00\n",
      "[[375   0]\n",
      " [  0 365]]\n",
      "True positive =  375\n",
      "False positive =  0\n",
      "False negative =  0\n",
      "True negative =  365\n",
      "Total Cost  0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       1.00      1.00      1.00       375\n",
      "       Good       1.00      1.00      1.00       365\n",
      "\n",
      "avg / total       1.00      1.00      1.00       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for Random forest model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_rf)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_rf,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_rf)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.85\n",
      "[[81  7]\n",
      " [20 78]]\n",
      "True positive =  81\n",
      "False positive =  20\n",
      "False negative =  7\n",
      "True negative =  78\n",
      "Total Cost  45000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.80      0.92      0.86        88\n",
      "       Good       0.92      0.80      0.85        98\n",
      "\n",
      "avg / total       0.86      0.85      0.85       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for Random forest model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_rf = rf_model.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_rf)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_rf,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_rf)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion = 'gini',max_depth= 4,max_leaf_nodes = 5)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.75\n",
      "[[346  29]\n",
      " [156 209]]\n",
      "True positive =  346\n",
      "False positive =  156\n",
      "False negative =  29\n",
      "True negative =  209\n",
      "Total Cost  223000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.69      0.92      0.79       375\n",
      "       Good       0.88      0.57      0.69       365\n",
      "\n",
      "avg / total       0.78      0.75      0.74       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for Decision Tree model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_clf = clf.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_clf)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_clf,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_clf)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.68\n",
      "[[78 10]\n",
      " [50 48]]\n",
      "True positive =  78\n",
      "False positive =  50\n",
      "False negative =  10\n",
      "True negative =  48\n",
      "Total Cost  75000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.61      0.89      0.72        88\n",
      "       Good       0.83      0.49      0.62        98\n",
      "\n",
      "avg / total       0.72      0.68      0.67       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for DT model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_clf = clf.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_clf)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_clf,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_clf)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad     224\n",
      "Good    176\n",
      "Name: Machine_State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#test predictions using DT model\n",
    "\n",
    "test_DT_pred = clf.predict(X_test)\n",
    "pred_test_DT = pd.DataFrame(test_DT_pred, columns=['Machine_State'])\n",
    "print(pd.value_counts(pred_test_DT['Machine_State']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search(Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "tree_para = {'criterion':['gini','entropy'],'max_depth':[3,4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150],\n",
    "             'min_samples_split': [2, 10, 20,25,30,35,40,50,60,70,80,90],'min_samples_leaf': [1,3, 5, 10,15],\n",
    "              'max_leaf_nodes': [None, 5, 10, 20]}\n",
    "GridDt = GridSearchCV(DecisionTreeClassifier(), tree_para, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=7, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150], 'min_samples_split': [2, 10, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90], 'min_samples_leaf': [1, 3, 5, 10, 15], 'max_leaf_nodes': [None, 5, 10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridDt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.97\n",
      "[[370   5]\n",
      " [ 18 347]]\n",
      "True positive =  370\n",
      "False positive =  18\n",
      "False negative =  5\n",
      "True negative =  347\n",
      "Total Cost  34000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.95      0.99      0.97       375\n",
      "       Good       0.99      0.95      0.97       365\n",
      "\n",
      "avg / total       0.97      0.97      0.97       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for Decision Tree model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_Grid = GridDt.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_Grid)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_Grid,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_Grid)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_Grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.73\n",
      "[[64 24]\n",
      " [26 72]]\n",
      "True positive =  64\n",
      "False positive =  26\n",
      "False negative =  24\n",
      "True negative =  72\n",
      "Total Cost  133000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.71      0.73      0.72        88\n",
      "       Good       0.75      0.73      0.74        98\n",
      "\n",
      "avg / total       0.73      0.73      0.73       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for DT model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_Grid = GridDt.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_Grid)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_Grid,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_Grid)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_Grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.98\n",
      "[[374   1]\n",
      " [ 14 351]]\n",
      "True positive =  374\n",
      "False positive =  14\n",
      "False negative =  1\n",
      "True negative =  351\n",
      "Total Cost  12000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.96      1.00      0.98       375\n",
      "       Good       1.00      0.96      0.98       365\n",
      "\n",
      "avg / total       0.98      0.98      0.98       740\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for XGBoost model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_XG = model.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_XG)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_XG,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_XG)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_XG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.88\n",
      "[[83  5]\n",
      " [17 81]]\n",
      "True positive =  83\n",
      "False positive =  17\n",
      "False negative =  5\n",
      "True negative =  81\n",
      "Total Cost  33500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.83      0.94      0.88        88\n",
      "       Good       0.94      0.83      0.88        98\n",
      "\n",
      "avg / total       0.89      0.88      0.88       186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for XGBoost model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_XG = model.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_XG)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_XG,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_XG)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_XG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search(XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alg = xgb.XGBClassifier(objective='binary:logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.26283201, -0.40616093, -0.37998002, -0.33013578, -0.34456587])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation.cross_val_score(alg,X_train,y_train, cv=5, scoring='log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    9.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8554054054054054, {'max_depth': 4, 'n_estimators': 200})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(alg,{'max_depth': [2,4,6],\n",
    "                        'n_estimators': [50,100,200]}, \n",
    "                        verbose=1, \n",
    "                        error_score='log_loss')\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "clf.best_score_, clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1.00\n",
      "[[375   0]\n",
      " [  0 365]]\n",
      "True positive =  375\n",
      "False positive =  0\n",
      "False negative =  0\n",
      "True negative =  365\n",
      "Total Cost  0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       1.00      1.00      1.00       375\n",
      "       Good       1.00      1.00      1.00       365\n",
      "\n",
      "avg / total       1.00      1.00      1.00       740\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for XGBoost model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_XG = clf.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_XG)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_XG,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_XG)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_XG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.87\n",
      "[[85  3]\n",
      " [21 77]]\n",
      "True positive =  85\n",
      "False positive =  21\n",
      "False negative =  3\n",
      "True negative =  77\n",
      "Total Cost  25500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.80      0.97      0.88        88\n",
      "       Good       0.96      0.79      0.87        98\n",
      "\n",
      "avg / total       0.89      0.87      0.87       186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Ramandeep\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for XGBoost model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_XG = clf.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_XG)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_XG,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_XG)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_XG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='ga',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='ga',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.86\n",
      "[[365  10]\n",
      " [ 94 271]]\n",
      "True positive =  365\n",
      "False positive =  94\n",
      "False negative =  10\n",
      "True negative =  271\n",
      "Total Cost  97000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.80      0.97      0.88       375\n",
      "       Good       0.96      0.74      0.84       365\n",
      "\n",
      "avg / total       0.88      0.86      0.86       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for Decision Tree model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_SVM = model.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_SVM)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_SVM,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_SVM)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.79\n",
      "[[82  6]\n",
      " [33 65]]\n",
      "True positive =  82\n",
      "False positive =  33\n",
      "False negative =  6\n",
      "True negative =  65\n",
      "Total Cost  46500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.71      0.93      0.81        88\n",
      "       Good       0.92      0.66      0.77        98\n",
      "\n",
      "avg / total       0.82      0.79      0.79       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for DT model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_SVM = model.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_SVM)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_SVM,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_SVM)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, grid_search\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.01}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_param_selection(X_train,y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelSVC = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='0.01', kernel='ga',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "   decision_function_shape='ovr', degree=3, gamma='0.01', kernel='ga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSVC.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.86\n",
      "[[365  10]\n",
      " [ 94 271]]\n",
      "True positive =  365\n",
      "False positive =  94\n",
      "False negative =  10\n",
      "True negative =  271\n",
      "Total Cost  97000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.80      0.97      0.88       375\n",
      "       Good       0.96      0.74      0.84       365\n",
      "\n",
      "avg / total       0.88      0.86      0.86       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for Decision Tree model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_SVC = modelSVC.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_SVC)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_SVC,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_SVC)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.79\n",
      "[[82  6]\n",
      " [33 65]]\n",
      "True positive =  82\n",
      "False positive =  33\n",
      "False negative =  6\n",
      "True negative =  65\n",
      "Total Cost  46500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.71      0.93      0.81        88\n",
      "       Good       0.92      0.66      0.77        98\n",
      "\n",
      "avg / total       0.82      0.79      0.79       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for DT model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_SVC = modelSVC.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_SVC)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_SVC,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_SVC)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
