{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#for plotting\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing # for standardizing numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Train & Test data\n",
    "Train_data = pd.read_csv(\"TrainingSet.csv\") \n",
    "Test_data = pd.read_csv(\"TestingSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Considering \"Bad\" as positive and \"Good\" as Negative because need to build a model that can detect a bad device\n",
    "\n",
    "cost_FP = 500 # cost of testing a good device classified as bad is $500\n",
    "cost_FN = 5000 # cost of testing a bad device classified as good is $5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dim  (3722, 220)\n",
      "Test dim  (400, 220)\n"
     ]
    }
   ],
   "source": [
    "#Checking dimensions of Train & Test\n",
    "print(\"Train dim \" , Train_data.shape)\n",
    "print(\"Test dim \" , Test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datatypes  V1               float64\n",
      "V2               float64\n",
      "V3               float64\n",
      "V4               float64\n",
      "V5               float64\n",
      "V6               float64\n",
      "V7               float64\n",
      "V8               float64\n",
      "V9               float64\n",
      "V10              float64\n",
      "V11              float64\n",
      "V12              float64\n",
      "V13              float64\n",
      "V14              float64\n",
      "V15              float64\n",
      "V16              float64\n",
      "V17              float64\n",
      "V18              float64\n",
      "V19              float64\n",
      "V20              float64\n",
      "V21              float64\n",
      "V22              float64\n",
      "V23              float64\n",
      "V24              float64\n",
      "V25              float64\n",
      "V26              float64\n",
      "V27              float64\n",
      "V28              float64\n",
      "V29              float64\n",
      "V30              float64\n",
      "                  ...   \n",
      "V191             float64\n",
      "V192             float64\n",
      "V193             float64\n",
      "V194             float64\n",
      "V195             float64\n",
      "V196             float64\n",
      "V197             float64\n",
      "V198             float64\n",
      "V199             float64\n",
      "V200             float64\n",
      "V201             float64\n",
      "V202             float64\n",
      "V203             float64\n",
      "V204             float64\n",
      "V205             float64\n",
      "V206             float64\n",
      "V207             float64\n",
      "V208             float64\n",
      "V209             float64\n",
      "V210             float64\n",
      "V211             float64\n",
      "V212             float64\n",
      "V213             float64\n",
      "V214             float64\n",
      "V215             float64\n",
      "V216             float64\n",
      "V217             float64\n",
      "V218             float64\n",
      "V219             float64\n",
      "Machine_State     object\n",
      "Length: 220, dtype: object\n",
      "Test datatypes  Sl No.      int64\n",
      "V1        float64\n",
      "V2        float64\n",
      "V3        float64\n",
      "V4        float64\n",
      "V5        float64\n",
      "V6        float64\n",
      "V7        float64\n",
      "V8        float64\n",
      "V9        float64\n",
      "V10       float64\n",
      "V11       float64\n",
      "V12       float64\n",
      "V13       float64\n",
      "V14       float64\n",
      "V15       float64\n",
      "V16       float64\n",
      "V17       float64\n",
      "V18       float64\n",
      "V19       float64\n",
      "V20       float64\n",
      "V21       float64\n",
      "V22       float64\n",
      "V23       float64\n",
      "V24       float64\n",
      "V25       float64\n",
      "V26       float64\n",
      "V27       float64\n",
      "V28       float64\n",
      "V29       float64\n",
      "           ...   \n",
      "V190      float64\n",
      "V191      float64\n",
      "V192      float64\n",
      "V193      float64\n",
      "V194      float64\n",
      "V195      float64\n",
      "V196      float64\n",
      "V197      float64\n",
      "V198      float64\n",
      "V199        int64\n",
      "V200      float64\n",
      "V201      float64\n",
      "V202      float64\n",
      "V203      float64\n",
      "V204      float64\n",
      "V205      float64\n",
      "V206      float64\n",
      "V207      float64\n",
      "V208      float64\n",
      "V209      float64\n",
      "V210      float64\n",
      "V211      float64\n",
      "V212      float64\n",
      "V213      float64\n",
      "V214      float64\n",
      "V215      float64\n",
      "V216      float64\n",
      "V217      float64\n",
      "V218      float64\n",
      "V219      float64\n",
      "Length: 220, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Train datatypes \" , Train_data.dtypes)\n",
    "print(\"Test datatypes \" , Test_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting Target Variable datatype to factor\n",
    "Train_data.Machine_State = Train_data.Machine_State.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good    3240\n",
      "Bad      463\n",
      "Name: Machine_State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking whether Response variable data distribution is balanced or not\n",
    "print(pd.value_counts(Train_data['Machine_State']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x216bee89630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x216beed6630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting Trget variable data distribution for visualization\n",
    "sns.countplot(x='Machine_State', data =Train_data,palette= 'hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1               56\n",
       "V8               56\n",
       "V15              56\n",
       "V22              56\n",
       "V29              56\n",
       "V36              56\n",
       "V43              56\n",
       "V50              56\n",
       "V57              56\n",
       "V64              56\n",
       "V71              56\n",
       "V78              56\n",
       "V85              56\n",
       "V92              56\n",
       "V99              56\n",
       "V106             56\n",
       "V113             56\n",
       "V120             56\n",
       "V127             56\n",
       "V134             56\n",
       "V141             56\n",
       "V148             56\n",
       "V155             56\n",
       "V162             56\n",
       "V169             56\n",
       "V176             56\n",
       "V183             56\n",
       "V190             56\n",
       "V197             56\n",
       "V205             56\n",
       "V213             56\n",
       "Machine_State    19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum of Null values per column on training data set\n",
    "null_columns=Train_data.columns[Train_data.isnull().any()]\n",
    "Train_data[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1      5\n",
       "V8      5\n",
       "V15     5\n",
       "V22     5\n",
       "V29     5\n",
       "V36     5\n",
       "V43     5\n",
       "V50     5\n",
       "V57     5\n",
       "V64     5\n",
       "V71     5\n",
       "V78     5\n",
       "V85     5\n",
       "V92     5\n",
       "V99     5\n",
       "V106    5\n",
       "V113    5\n",
       "V120    5\n",
       "V127    5\n",
       "V134    5\n",
       "V141    5\n",
       "V148    5\n",
       "V155    5\n",
       "V162    5\n",
       "V169    5\n",
       "V176    5\n",
       "V183    5\n",
       "V190    5\n",
       "V197    5\n",
       "V205    5\n",
       "V213    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sum of Null values per column on test data set\n",
    "null_columns=Test_data.columns[Test_data.isnull().any()]\n",
    "Test_data[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing Na with it's column mean\n",
    "Train_imputedSet=  Train_data.fillna(Train_data.mean())\n",
    "Test_imputedSet=  Test_data.fillna(Test_data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine_State    19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verifying if any null value exists for Train data\n",
    "null_columns=Train_imputedSet.columns[Train_imputedSet.isnull().any()]\n",
    "Train_imputedSet[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verifying if any null value exists for Test data\n",
    "null_columns=Test_imputedSet.columns[Test_imputedSet.isnull().any()]\n",
    "Test_imputedSet[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 220)\n",
      "Index(['Sl No.', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9',\n",
      "       ...\n",
      "       'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218',\n",
      "       'V219'],\n",
      "      dtype='object', length=220)\n"
     ]
    }
   ],
   "source": [
    "#Checking Test data dimension and column names\n",
    "print(Test_imputedSet.shape)\n",
    "print(Test_imputedSet.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 219)\n"
     ]
    }
   ],
   "source": [
    "#Remove the irrelevant column from Testdataset\n",
    "X_test = Test_imputedSet.loc[:,Test_imputedSet.columns != 'Sl No.']\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "3    4\n",
       "4    5\n",
       "Name: Sl No., dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To cocat with Submission file\n",
    "SerialNum = Test_imputedSet[\"Sl No.\"]\n",
    "SerialNum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace null values with 'Bad' as it is important and less too\n",
    "#Train_imputedSet['Machine_State'].fillna('Bad', inplace=True)\n",
    "\n",
    "#Drop NA target variable\n",
    "Train_imputedSet.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data to csv\n",
    "Train_imputedSet.to_csv(\"Imputed_DataSet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3703, 220)\n"
     ]
    }
   ],
   "source": [
    "print(Train_imputedSet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       ...\n",
      "       'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218',\n",
      "       'V219'],\n",
      "      dtype='object', length=219)\n",
      "(3703, 219)\n",
      "(3703, 1)\n"
     ]
    }
   ],
   "source": [
    "# Separating target variable from dataset\n",
    "X = Train_imputedSet.loc[:,Train_imputedSet.columns != 'Machine_State']\n",
    "Y = Train_imputedSet.loc[:,Train_imputedSet.columns == 'Machine_State']\n",
    "print(X.columns)  \n",
    "print(X.shape)  \n",
    "print(Y.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      V1        V2        V3        V4        V5        V6        V7  \\\n",
      "V1   NaN  0.164772  0.525073  0.149435  0.110333  0.164800  0.090617   \n",
      "V2   NaN       NaN  0.256672  0.972623  0.917249  1.000000  0.895930   \n",
      "V3   NaN       NaN       NaN  0.217200  0.126087  0.256713  0.083616   \n",
      "V4   NaN       NaN       NaN       NaN  0.969685  0.972630  0.914870   \n",
      "V5   NaN       NaN       NaN       NaN       NaN  0.917250  0.903655   \n",
      "V6   NaN       NaN       NaN       NaN       NaN       NaN  0.895913   \n",
      "V7   NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V8   NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V9   NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V10  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V11  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V12  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V13  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V14  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V15  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V16  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V17  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V18  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V19  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V20  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V21  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V22  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V23  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V24  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V25  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V26  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V27  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V28  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V29  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V30  NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "...   ..       ...       ...       ...       ...       ...       ...   \n",
      "V190 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V191 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V192 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V193 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V194 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V195 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V196 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V197 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V198 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V199 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V200 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V201 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V202 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V203 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V204 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V205 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V206 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V207 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V208 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V209 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V210 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V211 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V212 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V213 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V214 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V215 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V216 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V217 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V218 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "V219 NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "            V8        V9       V10    ...         V210      V211      V212  \\\n",
      "V1    0.999732  0.165216  0.524744    ...     0.040112  0.084196  0.052363   \n",
      "V2    0.164710  0.999009  0.256499    ...     0.064770  0.000694  0.202903   \n",
      "V3    0.524783  0.259405  0.999943    ...     0.042598  0.067780  0.091373   \n",
      "V4    0.149451  0.969425  0.217074    ...     0.065800  0.001104  0.229585   \n",
      "V5    0.110447  0.913995  0.126012    ...     0.056630  0.005898  0.235516   \n",
      "V6    0.164738  0.999010  0.256540    ...     0.064759  0.000693  0.203009   \n",
      "V7    0.090636  0.897150  0.083494    ...     0.046679  0.006766  0.197513   \n",
      "V8         NaN  0.165146  0.524558    ...     0.039829  0.084934  0.052332   \n",
      "V9         NaN       NaN  0.259232    ...     0.064858  0.001710  0.202595   \n",
      "V10        NaN       NaN       NaN    ...     0.042476  0.067939  0.091548   \n",
      "V11        NaN       NaN       NaN    ...     0.066293  0.002348  0.229988   \n",
      "V12        NaN       NaN       NaN    ...     0.056482  0.009460  0.238679   \n",
      "V13        NaN       NaN       NaN    ...     0.064848  0.001709  0.202703   \n",
      "V14        NaN       NaN       NaN    ...     0.047127  0.004371  0.191895   \n",
      "V15        NaN       NaN       NaN    ...     0.042531  0.082960  0.053155   \n",
      "V16        NaN       NaN       NaN    ...     0.051606  0.026528  0.221251   \n",
      "V17        NaN       NaN       NaN    ...     0.042054  0.071945  0.101217   \n",
      "V18        NaN       NaN       NaN    ...     0.052232  0.038025  0.249166   \n",
      "V19        NaN       NaN       NaN    ...     0.037614  0.048256  0.252408   \n",
      "V20        NaN       NaN       NaN    ...     0.051594  0.026527  0.221378   \n",
      "V21        NaN       NaN       NaN    ...     0.033961  0.038076  0.199819   \n",
      "V22        NaN       NaN       NaN    ...     0.039550  0.093115  0.056290   \n",
      "V23        NaN       NaN       NaN    ...     0.053496  0.032668  0.222347   \n",
      "V24        NaN       NaN       NaN    ...     0.042654  0.070832  0.102028   \n",
      "V25        NaN       NaN       NaN    ...     0.054186  0.045554  0.251204   \n",
      "V26        NaN       NaN       NaN    ...     0.039846  0.059941  0.257617   \n",
      "V27        NaN       NaN       NaN    ...     0.053484  0.032666  0.222472   \n",
      "V28        NaN       NaN       NaN    ...     0.035282  0.045515  0.200197   \n",
      "V29        NaN       NaN       NaN    ...     0.019239  0.046070  0.074172   \n",
      "V30        NaN       NaN       NaN    ...     0.059140  0.037241  0.084751   \n",
      "...        ...       ...       ...    ...          ...       ...       ...   \n",
      "V190       NaN       NaN       NaN    ...     0.051337  0.174397  0.042392   \n",
      "V191       NaN       NaN       NaN    ...     0.060134  0.087338  0.123982   \n",
      "V192       NaN       NaN       NaN    ...     0.083656  0.131988  0.083829   \n",
      "V193       NaN       NaN       NaN    ...     0.067227  0.091653  0.152993   \n",
      "V194       NaN       NaN       NaN    ...     0.062044  0.090891  0.169007   \n",
      "V195       NaN       NaN       NaN    ...     0.060131  0.087346  0.124059   \n",
      "V196       NaN       NaN       NaN    ...     0.048150  0.072454  0.123000   \n",
      "V197       NaN       NaN       NaN    ...     0.019948  0.007017  0.000730   \n",
      "V198       NaN       NaN       NaN    ...     0.003758  0.011843  0.027009   \n",
      "V199       NaN       NaN       NaN    ...     0.007538  0.016679  0.002560   \n",
      "V200       NaN       NaN       NaN    ...     0.021584  0.007521  0.000792   \n",
      "V201       NaN       NaN       NaN    ...     0.023142  0.003122  0.005054   \n",
      "V202       NaN       NaN       NaN    ...     0.003756  0.011845  0.027012   \n",
      "V203       NaN       NaN       NaN    ...     0.023065  0.000857  0.000334   \n",
      "V204       NaN       NaN       NaN    ...     0.059110  0.079245  0.351837   \n",
      "V205       NaN       NaN       NaN    ...     0.950692  0.135782  0.041850   \n",
      "V206       NaN       NaN       NaN    ...     1.000000  0.098802  0.022957   \n",
      "V207       NaN       NaN       NaN    ...     0.967944  0.185086  0.090997   \n",
      "V208       NaN       NaN       NaN    ...     0.945325  0.081761  0.133637   \n",
      "V209       NaN       NaN       NaN    ...     0.014027  0.483422  0.427664   \n",
      "V210       NaN       NaN       NaN    ...          NaN  0.098807  0.022955   \n",
      "V211       NaN       NaN       NaN    ...          NaN       NaN  0.101817   \n",
      "V212       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V213       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V214       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V215       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V216       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V217       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V218       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "V219       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
      "\n",
      "          V213      V214      V215      V216      V217      V218      V219  \n",
      "V1    0.076290  0.084725  0.012060  0.132257  0.169221  0.084727  0.021924  \n",
      "V2    0.011735  0.017402  0.098983  0.065286  0.199041  0.017410  0.003845  \n",
      "V3    0.048549  0.057725  0.037111  0.067928  0.052502  0.057724  0.067797  \n",
      "V4    0.015277  0.021072  0.082058  0.044102  0.151328  0.021077  0.002186  \n",
      "V5    0.003218  0.009069  0.059931  0.044117  0.124826  0.009079  0.001087  \n",
      "V6    0.011721  0.017387  0.098945  0.065274  0.198981  0.017395  0.003833  \n",
      "V7    0.002454  0.002262  0.073962  0.069266  0.173573  0.002278  0.011067  \n",
      "V8    0.076258  0.084708  0.012118  0.132172  0.169028  0.084710  0.021993  \n",
      "V9    0.011093  0.016819  0.101180  0.068218  0.205587  0.016828  0.003332  \n",
      "V10   0.048534  0.057725  0.037151  0.067911  0.052394  0.057725  0.067780  \n",
      "V11   0.014768  0.020831  0.084846  0.047186  0.158686  0.020836  0.001545  \n",
      "V12   0.001944  0.007828  0.062102  0.048278  0.133217  0.007839  0.000671  \n",
      "V13   0.011079  0.016804  0.101142  0.068205  0.205525  0.016813  0.003319  \n",
      "V14   0.003558  0.001055  0.075262  0.072432  0.179294  0.001072  0.010864  \n",
      "V15   0.074859  0.082587  0.008988  0.131336  0.171109  0.082589  0.019655  \n",
      "V16   0.002966  0.007486  0.109501  0.090846  0.251342  0.007496  0.010088  \n",
      "V17   0.047154  0.054293  0.033548  0.064447  0.051392  0.054291  0.061066  \n",
      "V18   0.004100  0.006037  0.089368  0.074783  0.205926  0.006041  0.016096  \n",
      "V19   0.009984  0.008496  0.062571  0.073996  0.173111  0.008487  0.014257  \n",
      "V20   0.002947  0.007469  0.109463  0.090836  0.251283  0.007478  0.010066  \n",
      "V21   0.013579  0.010804  0.082110  0.097600  0.227285  0.010788  0.020403  \n",
      "V22   0.076823  0.084749  0.009424  0.134584  0.175162  0.084750  0.020322  \n",
      "V23   0.001261  0.005093  0.108896  0.094650  0.256646  0.005106  0.013139  \n",
      "V24   0.047286  0.055804  0.033406  0.066763  0.055611  0.055803  0.057050  \n",
      "V25   0.001037  0.002169  0.088215  0.081951  0.214717  0.002175  0.019012  \n",
      "V26   0.012174  0.012390  0.061404  0.080930  0.181742  0.012380  0.018934  \n",
      "V27   0.001243  0.005077  0.108858  0.094640  0.256588  0.005089  0.013114  \n",
      "V28   0.018433  0.017061  0.078265  0.107037  0.237012  0.017044  0.023864  \n",
      "V29   0.048482  0.062133  0.062454  0.149180  0.247755  0.062081  0.068028  \n",
      "V30   0.080441  0.101149  0.029884  0.186483  0.259644  0.101096  0.071727  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "V190  0.123842  0.145890  0.002333  0.245435  0.331893  0.145890  0.027830  \n",
      "V191  0.115481  0.129556  0.086879  0.142686  0.105049  0.129572  0.002733  \n",
      "V192  0.127466  0.151325  0.058883  0.203505  0.208821  0.151323  0.026166  \n",
      "V193  0.123544  0.140777  0.079372  0.168199  0.147319  0.140792  0.012251  \n",
      "V194  0.112951  0.129694  0.057852  0.168698  0.173523  0.129713  0.014510  \n",
      "V195  0.115494  0.129566  0.086855  0.142728  0.105136  0.129582  0.002736  \n",
      "V196  0.088664  0.098263  0.059414  0.115572  0.101307  0.098285  0.002738  \n",
      "V197  0.030884  0.026630  0.022403  0.031933  0.016834  0.026629  0.005745  \n",
      "V198  0.012190  0.012754  0.001497  0.021374  0.026940  0.012753  0.000562  \n",
      "V199  0.007833  0.008792  0.001738  0.016612  0.024390  0.008792  0.000427  \n",
      "V200  0.032865  0.029150  0.023494  0.035112  0.019517  0.029146  0.006956  \n",
      "V201  0.029099  0.025845  0.021216  0.030943  0.016481  0.025841  0.006259  \n",
      "V202  0.012189  0.012752  0.001497  0.021371  0.026937  0.012751  0.000561  \n",
      "V203  0.030840  0.025808  0.026232  0.027471  0.006051  0.025805  0.007599  \n",
      "V204  0.213187  0.246518  0.014024  0.413651  0.550619  0.246531  0.047031  \n",
      "V205  0.523008  0.490811  0.496876  0.421907  0.009213  0.490815  0.041430  \n",
      "V206  0.513967  0.538320  0.526754  0.468478  0.037399  0.538321  0.062149  \n",
      "V207  0.508052  0.527187  0.540251  0.448597  0.009615  0.527187  0.070690  \n",
      "V208  0.484766  0.502587  0.480934  0.456234  0.081889  0.502587  0.054473  \n",
      "V209  0.007834  0.009989  0.077752  0.048973  0.205056  0.009988  0.020652  \n",
      "V210  0.514004  0.538356  0.526788  0.468512  0.037408  0.538357  0.062162  \n",
      "V211  0.008710  0.125367  0.070923  0.150563  0.135230  0.125367  0.450467  \n",
      "V212  0.022355  0.030957  0.014505  0.068896  0.107369  0.030947  0.022132  \n",
      "V213       NaN  0.947699  0.875995  0.885793  0.208010  0.947705  0.080118  \n",
      "V214       NaN       NaN  0.908723  0.932182  0.235421  1.000000  0.133787  \n",
      "V215       NaN       NaN       NaN  0.719139  0.163463  0.908729  0.140784  \n",
      "V216       NaN       NaN       NaN       NaN  0.551833  0.932178  0.115358  \n",
      "V217       NaN       NaN       NaN       NaN       NaN  0.235413  0.027440  \n",
      "V218       NaN       NaN       NaN       NaN       NaN       NaN  0.133795  \n",
      "V219       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "\n",
      "[219 rows x 219 columns]\n"
     ]
    }
   ],
   "source": [
    "#Check correlation\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix ( Nan represents highly correlation between the variables)\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "print(upper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n",
      "['V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V30', 'V31', 'V32', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V65', 'V67', 'V68', 'V69', 'V70', 'V73', 'V74', 'V76', 'V79', 'V80', 'V81', 'V83', 'V87', 'V88', 'V89', 'V90', 'V91', 'V96', 'V97', 'V101', 'V103', 'V104', 'V107', 'V110', 'V111', 'V115', 'V117', 'V118', 'V125', 'V128', 'V132', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V156', 'V157', 'V158', 'V160', 'V163', 'V164', 'V165', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V199', 'V200', 'V201', 'V202', 'V203', 'V205', 'V206', 'V207', 'V208', 'V210', 'V214', 'V215', 'V216', 'V218']\n"
     ]
    }
   ],
   "source": [
    "#EXtract the columns which are highly coorelated \n",
    "\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "print(len(to_drop))\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the highly correlated columns\n",
    "dropped_df = X.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3703, 61)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the dimensions after dropping\n",
    "dropped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V29</th>\n",
       "      <th>V33</th>\n",
       "      <th>V57</th>\n",
       "      <th>V64</th>\n",
       "      <th>V66</th>\n",
       "      <th>V71</th>\n",
       "      <th>V72</th>\n",
       "      <th>...</th>\n",
       "      <th>V168</th>\n",
       "      <th>V197</th>\n",
       "      <th>V198</th>\n",
       "      <th>V204</th>\n",
       "      <th>V209</th>\n",
       "      <th>V211</th>\n",
       "      <th>V212</th>\n",
       "      <th>V213</th>\n",
       "      <th>V217</th>\n",
       "      <th>V219</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164772</td>\n",
       "      <td>0.525073</td>\n",
       "      <td>0.078567</td>\n",
       "      <td>0.060589</td>\n",
       "      <td>0.021213</td>\n",
       "      <td>0.588236</td>\n",
       "      <td>0.243176</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.005494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035978</td>\n",
       "      <td>0.013728</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.026723</td>\n",
       "      <td>0.084196</td>\n",
       "      <td>0.052363</td>\n",
       "      <td>0.076290</td>\n",
       "      <td>0.169221</td>\n",
       "      <td>0.021924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>0.164772</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.256672</td>\n",
       "      <td>0.252714</td>\n",
       "      <td>0.714931</td>\n",
       "      <td>0.025463</td>\n",
       "      <td>0.162511</td>\n",
       "      <td>0.346018</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.141455</td>\n",
       "      <td>0.059831</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.202903</td>\n",
       "      <td>0.011735</td>\n",
       "      <td>0.199041</td>\n",
       "      <td>0.003845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>0.525073</td>\n",
       "      <td>0.256672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031332</td>\n",
       "      <td>0.111155</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.420424</td>\n",
       "      <td>0.532158</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032005</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.053341</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.067780</td>\n",
       "      <td>0.091373</td>\n",
       "      <td>0.048549</td>\n",
       "      <td>0.052502</td>\n",
       "      <td>0.067797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V29</th>\n",
       "      <td>0.078567</td>\n",
       "      <td>0.252714</td>\n",
       "      <td>0.031332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395709</td>\n",
       "      <td>0.267549</td>\n",
       "      <td>0.092134</td>\n",
       "      <td>0.060104</td>\n",
       "      <td>0.045258</td>\n",
       "      <td>0.048409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041013</td>\n",
       "      <td>0.020839</td>\n",
       "      <td>0.017295</td>\n",
       "      <td>0.047651</td>\n",
       "      <td>0.183742</td>\n",
       "      <td>0.046070</td>\n",
       "      <td>0.074172</td>\n",
       "      <td>0.048482</td>\n",
       "      <td>0.247755</td>\n",
       "      <td>0.068028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V33</th>\n",
       "      <td>0.060589</td>\n",
       "      <td>0.714931</td>\n",
       "      <td>0.111155</td>\n",
       "      <td>0.395709</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.254586</td>\n",
       "      <td>0.185031</td>\n",
       "      <td>0.439326</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036678</td>\n",
       "      <td>0.008921</td>\n",
       "      <td>0.032319</td>\n",
       "      <td>0.151183</td>\n",
       "      <td>0.096099</td>\n",
       "      <td>0.103397</td>\n",
       "      <td>0.033966</td>\n",
       "      <td>0.199822</td>\n",
       "      <td>0.034295</td>\n",
       "      <td>0.033442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V57</th>\n",
       "      <td>0.021213</td>\n",
       "      <td>0.025463</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.267549</td>\n",
       "      <td>0.254586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033783</td>\n",
       "      <td>0.036850</td>\n",
       "      <td>0.056106</td>\n",
       "      <td>0.057690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.097599</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.043464</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>0.053557</td>\n",
       "      <td>0.388307</td>\n",
       "      <td>0.018967</td>\n",
       "      <td>0.021013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V64</th>\n",
       "      <td>0.588236</td>\n",
       "      <td>0.162511</td>\n",
       "      <td>0.420424</td>\n",
       "      <td>0.092134</td>\n",
       "      <td>0.185031</td>\n",
       "      <td>0.033783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.633490</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.007736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045332</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.170276</td>\n",
       "      <td>0.176821</td>\n",
       "      <td>0.309185</td>\n",
       "      <td>0.066612</td>\n",
       "      <td>0.100480</td>\n",
       "      <td>0.273925</td>\n",
       "      <td>0.013924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V66</th>\n",
       "      <td>0.243176</td>\n",
       "      <td>0.346018</td>\n",
       "      <td>0.532158</td>\n",
       "      <td>0.060104</td>\n",
       "      <td>0.439326</td>\n",
       "      <td>0.036850</td>\n",
       "      <td>0.633490</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.027374</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>0.117034</td>\n",
       "      <td>0.112182</td>\n",
       "      <td>0.187271</td>\n",
       "      <td>0.086840</td>\n",
       "      <td>0.070597</td>\n",
       "      <td>0.068254</td>\n",
       "      <td>0.005531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V71</th>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.045258</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>0.056106</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101132</td>\n",
       "      <td>0.010749</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>0.020632</td>\n",
       "      <td>0.039980</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.031857</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.022176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V72</th>\n",
       "      <td>0.005494</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.048409</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.057690</td>\n",
       "      <td>0.007736</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>0.724406</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109543</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.045686</td>\n",
       "      <td>0.013939</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.003168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V75</th>\n",
       "      <td>0.023095</td>\n",
       "      <td>0.075923</td>\n",
       "      <td>0.042176</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.017992</td>\n",
       "      <td>0.039186</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.037279</td>\n",
       "      <td>0.058321</td>\n",
       "      <td>0.019759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064623</td>\n",
       "      <td>0.031676</td>\n",
       "      <td>0.011425</td>\n",
       "      <td>0.092412</td>\n",
       "      <td>0.068151</td>\n",
       "      <td>0.004465</td>\n",
       "      <td>0.072529</td>\n",
       "      <td>0.015356</td>\n",
       "      <td>0.073781</td>\n",
       "      <td>0.013252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V77</th>\n",
       "      <td>0.023637</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.021278</td>\n",
       "      <td>0.060634</td>\n",
       "      <td>0.018548</td>\n",
       "      <td>0.038750</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.577050</td>\n",
       "      <td>0.054681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293418</td>\n",
       "      <td>0.005820</td>\n",
       "      <td>0.007512</td>\n",
       "      <td>0.043695</td>\n",
       "      <td>0.031890</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>0.012540</td>\n",
       "      <td>0.029732</td>\n",
       "      <td>0.028865</td>\n",
       "      <td>0.038583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V78</th>\n",
       "      <td>0.020282</td>\n",
       "      <td>0.012507</td>\n",
       "      <td>0.013327</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.004248</td>\n",
       "      <td>0.019306</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>0.385538</td>\n",
       "      <td>0.252957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.025601</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.010073</td>\n",
       "      <td>0.016404</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.002567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V82</th>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.148171</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.318393</td>\n",
       "      <td>0.119526</td>\n",
       "      <td>0.061292</td>\n",
       "      <td>0.004502</td>\n",
       "      <td>0.079028</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.024441</td>\n",
       "      <td>0.013781</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.029772</td>\n",
       "      <td>0.025725</td>\n",
       "      <td>0.139273</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>0.113540</td>\n",
       "      <td>0.009942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V84</th>\n",
       "      <td>0.029294</td>\n",
       "      <td>0.013868</td>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.035791</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.023708</td>\n",
       "      <td>0.047744</td>\n",
       "      <td>0.024668</td>\n",
       "      <td>0.255923</td>\n",
       "      <td>0.069510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024003</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.003014</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.029089</td>\n",
       "      <td>0.013675</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.011632</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.028037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V85</th>\n",
       "      <td>0.016823</td>\n",
       "      <td>0.006358</td>\n",
       "      <td>0.013461</td>\n",
       "      <td>0.028996</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.083893</td>\n",
       "      <td>0.025851</td>\n",
       "      <td>0.025404</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.775048</td>\n",
       "      <td>0.354298</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.045251</td>\n",
       "      <td>0.010923</td>\n",
       "      <td>0.029905</td>\n",
       "      <td>0.034153</td>\n",
       "      <td>0.014977</td>\n",
       "      <td>0.006764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V86</th>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.019905</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.026897</td>\n",
       "      <td>0.029842</td>\n",
       "      <td>0.093244</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.014879</td>\n",
       "      <td>0.014608</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.461420</td>\n",
       "      <td>0.662646</td>\n",
       "      <td>0.024028</td>\n",
       "      <td>0.065741</td>\n",
       "      <td>0.015831</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>0.013557</td>\n",
       "      <td>0.027293</td>\n",
       "      <td>0.000906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V92</th>\n",
       "      <td>0.007852</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>0.016816</td>\n",
       "      <td>0.288594</td>\n",
       "      <td>0.050848</td>\n",
       "      <td>0.143639</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.116916</td>\n",
       "      <td>0.098512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>0.033678</td>\n",
       "      <td>0.050302</td>\n",
       "      <td>0.108412</td>\n",
       "      <td>0.073215</td>\n",
       "      <td>0.077663</td>\n",
       "      <td>0.054456</td>\n",
       "      <td>0.062071</td>\n",
       "      <td>0.018570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V93</th>\n",
       "      <td>0.018843</td>\n",
       "      <td>0.575762</td>\n",
       "      <td>0.072405</td>\n",
       "      <td>0.401571</td>\n",
       "      <td>0.693993</td>\n",
       "      <td>0.083188</td>\n",
       "      <td>0.134149</td>\n",
       "      <td>0.374343</td>\n",
       "      <td>0.067785</td>\n",
       "      <td>0.136332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036538</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.022605</td>\n",
       "      <td>0.122997</td>\n",
       "      <td>0.032451</td>\n",
       "      <td>0.064693</td>\n",
       "      <td>0.142165</td>\n",
       "      <td>0.029419</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.059523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V94</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.087225</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.301320</td>\n",
       "      <td>0.048457</td>\n",
       "      <td>0.067697</td>\n",
       "      <td>0.012975</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>0.018154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.011274</td>\n",
       "      <td>0.024640</td>\n",
       "      <td>0.083110</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>0.052883</td>\n",
       "      <td>0.115318</td>\n",
       "      <td>0.131930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V95</th>\n",
       "      <td>0.023250</td>\n",
       "      <td>0.528391</td>\n",
       "      <td>0.054079</td>\n",
       "      <td>0.059608</td>\n",
       "      <td>0.546288</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.103598</td>\n",
       "      <td>0.306761</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.058793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015530</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>0.072355</td>\n",
       "      <td>0.013633</td>\n",
       "      <td>0.020417</td>\n",
       "      <td>0.089509</td>\n",
       "      <td>0.052611</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>0.007597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V98</th>\n",
       "      <td>0.015292</td>\n",
       "      <td>0.399063</td>\n",
       "      <td>0.028466</td>\n",
       "      <td>0.187630</td>\n",
       "      <td>0.407637</td>\n",
       "      <td>0.035857</td>\n",
       "      <td>0.078927</td>\n",
       "      <td>0.208071</td>\n",
       "      <td>0.019319</td>\n",
       "      <td>0.033173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009764</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.069142</td>\n",
       "      <td>0.007192</td>\n",
       "      <td>0.019643</td>\n",
       "      <td>0.106789</td>\n",
       "      <td>0.019617</td>\n",
       "      <td>0.026385</td>\n",
       "      <td>0.002817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V99</th>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.022274</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.215299</td>\n",
       "      <td>0.027630</td>\n",
       "      <td>0.137521</td>\n",
       "      <td>0.009413</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.174382</td>\n",
       "      <td>0.145782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>0.024117</td>\n",
       "      <td>0.031120</td>\n",
       "      <td>0.043814</td>\n",
       "      <td>0.093463</td>\n",
       "      <td>0.040244</td>\n",
       "      <td>0.059480</td>\n",
       "      <td>0.041967</td>\n",
       "      <td>0.043075</td>\n",
       "      <td>0.009439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V100</th>\n",
       "      <td>0.036947</td>\n",
       "      <td>0.569314</td>\n",
       "      <td>0.076910</td>\n",
       "      <td>0.308130</td>\n",
       "      <td>0.624120</td>\n",
       "      <td>0.080993</td>\n",
       "      <td>0.115139</td>\n",
       "      <td>0.315364</td>\n",
       "      <td>0.058269</td>\n",
       "      <td>0.105753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046823</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.129573</td>\n",
       "      <td>0.026293</td>\n",
       "      <td>0.045778</td>\n",
       "      <td>0.193899</td>\n",
       "      <td>0.040689</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.065743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V102</th>\n",
       "      <td>0.031435</td>\n",
       "      <td>0.519671</td>\n",
       "      <td>0.051189</td>\n",
       "      <td>0.055283</td>\n",
       "      <td>0.474846</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>0.070052</td>\n",
       "      <td>0.235063</td>\n",
       "      <td>0.017144</td>\n",
       "      <td>0.051797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027563</td>\n",
       "      <td>0.012152</td>\n",
       "      <td>0.009723</td>\n",
       "      <td>0.116441</td>\n",
       "      <td>0.070824</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.182974</td>\n",
       "      <td>0.052991</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.007429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V105</th>\n",
       "      <td>0.007914</td>\n",
       "      <td>0.413909</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.231107</td>\n",
       "      <td>0.416898</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.020652</td>\n",
       "      <td>0.169926</td>\n",
       "      <td>0.033811</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076633</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.007346</td>\n",
       "      <td>0.113695</td>\n",
       "      <td>0.050977</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.183647</td>\n",
       "      <td>0.026750</td>\n",
       "      <td>0.011816</td>\n",
       "      <td>0.019131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V106</th>\n",
       "      <td>0.006670</td>\n",
       "      <td>0.011513</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.110009</td>\n",
       "      <td>0.040599</td>\n",
       "      <td>0.095905</td>\n",
       "      <td>0.009169</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.032458</td>\n",
       "      <td>0.025747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015560</td>\n",
       "      <td>0.021121</td>\n",
       "      <td>0.017707</td>\n",
       "      <td>0.036817</td>\n",
       "      <td>0.057050</td>\n",
       "      <td>0.020206</td>\n",
       "      <td>0.031203</td>\n",
       "      <td>0.019541</td>\n",
       "      <td>0.035629</td>\n",
       "      <td>0.015729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V108</th>\n",
       "      <td>0.004410</td>\n",
       "      <td>0.155660</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.271191</td>\n",
       "      <td>0.094728</td>\n",
       "      <td>0.076744</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>0.063870</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021889</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.024975</td>\n",
       "      <td>0.044948</td>\n",
       "      <td>0.070680</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>0.065618</td>\n",
       "      <td>0.101455</td>\n",
       "      <td>0.125820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V109</th>\n",
       "      <td>0.011598</td>\n",
       "      <td>0.503818</td>\n",
       "      <td>0.053433</td>\n",
       "      <td>0.092785</td>\n",
       "      <td>0.602260</td>\n",
       "      <td>0.021091</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.283858</td>\n",
       "      <td>0.028431</td>\n",
       "      <td>0.071750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019684</td>\n",
       "      <td>0.018089</td>\n",
       "      <td>0.020947</td>\n",
       "      <td>0.090694</td>\n",
       "      <td>0.032433</td>\n",
       "      <td>0.013345</td>\n",
       "      <td>0.051816</td>\n",
       "      <td>0.057092</td>\n",
       "      <td>0.032216</td>\n",
       "      <td>0.008461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V112</th>\n",
       "      <td>0.014384</td>\n",
       "      <td>0.438434</td>\n",
       "      <td>0.017430</td>\n",
       "      <td>0.285606</td>\n",
       "      <td>0.532213</td>\n",
       "      <td>0.013713</td>\n",
       "      <td>0.072018</td>\n",
       "      <td>0.208409</td>\n",
       "      <td>0.041159</td>\n",
       "      <td>0.039403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045597</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>0.100512</td>\n",
       "      <td>0.021476</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.101349</td>\n",
       "      <td>0.022670</td>\n",
       "      <td>0.013104</td>\n",
       "      <td>0.028031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V114</th>\n",
       "      <td>0.055756</td>\n",
       "      <td>0.578957</td>\n",
       "      <td>0.108840</td>\n",
       "      <td>0.355018</td>\n",
       "      <td>0.771736</td>\n",
       "      <td>0.097048</td>\n",
       "      <td>0.153839</td>\n",
       "      <td>0.355882</td>\n",
       "      <td>0.084732</td>\n",
       "      <td>0.139246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017420</td>\n",
       "      <td>0.041035</td>\n",
       "      <td>0.051812</td>\n",
       "      <td>0.091002</td>\n",
       "      <td>0.079052</td>\n",
       "      <td>0.077040</td>\n",
       "      <td>0.061979</td>\n",
       "      <td>0.013143</td>\n",
       "      <td>0.024999</td>\n",
       "      <td>0.083265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V116</th>\n",
       "      <td>0.021234</td>\n",
       "      <td>0.431458</td>\n",
       "      <td>0.054499</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.497771</td>\n",
       "      <td>0.014346</td>\n",
       "      <td>0.093128</td>\n",
       "      <td>0.253894</td>\n",
       "      <td>0.027339</td>\n",
       "      <td>0.081025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>0.020440</td>\n",
       "      <td>0.104428</td>\n",
       "      <td>0.022399</td>\n",
       "      <td>0.014718</td>\n",
       "      <td>0.074737</td>\n",
       "      <td>0.061194</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.025065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V119</th>\n",
       "      <td>0.009473</td>\n",
       "      <td>0.453095</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.290224</td>\n",
       "      <td>0.556250</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.070666</td>\n",
       "      <td>0.206058</td>\n",
       "      <td>0.049685</td>\n",
       "      <td>0.025991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059429</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>0.103174</td>\n",
       "      <td>0.023239</td>\n",
       "      <td>0.021707</td>\n",
       "      <td>0.087009</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>0.009981</td>\n",
       "      <td>0.051464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V120</th>\n",
       "      <td>0.024479</td>\n",
       "      <td>0.123279</td>\n",
       "      <td>0.035893</td>\n",
       "      <td>0.102624</td>\n",
       "      <td>0.218547</td>\n",
       "      <td>0.224462</td>\n",
       "      <td>0.117536</td>\n",
       "      <td>0.168481</td>\n",
       "      <td>0.042805</td>\n",
       "      <td>0.024601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.330656</td>\n",
       "      <td>0.288206</td>\n",
       "      <td>0.118536</td>\n",
       "      <td>0.034621</td>\n",
       "      <td>0.031875</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.158258</td>\n",
       "      <td>0.056918</td>\n",
       "      <td>0.004132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V121</th>\n",
       "      <td>0.043232</td>\n",
       "      <td>0.115313</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>0.030549</td>\n",
       "      <td>0.155389</td>\n",
       "      <td>0.109730</td>\n",
       "      <td>0.032174</td>\n",
       "      <td>0.069082</td>\n",
       "      <td>0.031589</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010394</td>\n",
       "      <td>0.238220</td>\n",
       "      <td>0.305404</td>\n",
       "      <td>0.215628</td>\n",
       "      <td>0.135390</td>\n",
       "      <td>0.107591</td>\n",
       "      <td>0.100124</td>\n",
       "      <td>0.150176</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>0.004020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V122</th>\n",
       "      <td>0.039066</td>\n",
       "      <td>0.034196</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.132005</td>\n",
       "      <td>0.061508</td>\n",
       "      <td>0.045776</td>\n",
       "      <td>0.183446</td>\n",
       "      <td>0.075541</td>\n",
       "      <td>0.012944</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022755</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.097070</td>\n",
       "      <td>0.064537</td>\n",
       "      <td>0.174386</td>\n",
       "      <td>0.250981</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.052128</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.001032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V123</th>\n",
       "      <td>0.037099</td>\n",
       "      <td>0.265170</td>\n",
       "      <td>0.072702</td>\n",
       "      <td>0.062577</td>\n",
       "      <td>0.304482</td>\n",
       "      <td>0.146078</td>\n",
       "      <td>0.179327</td>\n",
       "      <td>0.261780</td>\n",
       "      <td>0.042441</td>\n",
       "      <td>0.023787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019876</td>\n",
       "      <td>0.305268</td>\n",
       "      <td>0.290885</td>\n",
       "      <td>0.220502</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>0.056134</td>\n",
       "      <td>0.115467</td>\n",
       "      <td>0.200473</td>\n",
       "      <td>0.133672</td>\n",
       "      <td>0.009966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V124</th>\n",
       "      <td>0.048836</td>\n",
       "      <td>0.129674</td>\n",
       "      <td>0.041528</td>\n",
       "      <td>0.183441</td>\n",
       "      <td>0.136754</td>\n",
       "      <td>0.054171</td>\n",
       "      <td>0.273243</td>\n",
       "      <td>0.231945</td>\n",
       "      <td>0.019057</td>\n",
       "      <td>0.020687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038141</td>\n",
       "      <td>0.218596</td>\n",
       "      <td>0.170365</td>\n",
       "      <td>0.118602</td>\n",
       "      <td>0.120661</td>\n",
       "      <td>0.256952</td>\n",
       "      <td>0.068526</td>\n",
       "      <td>0.102666</td>\n",
       "      <td>0.106746</td>\n",
       "      <td>0.008345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V126</th>\n",
       "      <td>0.035973</td>\n",
       "      <td>0.028308</td>\n",
       "      <td>0.029669</td>\n",
       "      <td>0.190180</td>\n",
       "      <td>0.076719</td>\n",
       "      <td>0.158620</td>\n",
       "      <td>0.095549</td>\n",
       "      <td>0.113448</td>\n",
       "      <td>0.012374</td>\n",
       "      <td>0.015503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033694</td>\n",
       "      <td>0.280281</td>\n",
       "      <td>0.151684</td>\n",
       "      <td>0.081406</td>\n",
       "      <td>0.166708</td>\n",
       "      <td>0.067934</td>\n",
       "      <td>0.129529</td>\n",
       "      <td>0.006328</td>\n",
       "      <td>0.036173</td>\n",
       "      <td>0.022472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V127</th>\n",
       "      <td>0.014733</td>\n",
       "      <td>0.096063</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.083801</td>\n",
       "      <td>0.058743</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.028779</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.024965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053291</td>\n",
       "      <td>0.216594</td>\n",
       "      <td>0.282699</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.085018</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.177310</td>\n",
       "      <td>0.026371</td>\n",
       "      <td>0.098456</td>\n",
       "      <td>0.001871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V129</th>\n",
       "      <td>0.009710</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.031492</td>\n",
       "      <td>0.068995</td>\n",
       "      <td>0.038607</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.010713</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.012187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017581</td>\n",
       "      <td>0.010661</td>\n",
       "      <td>0.025816</td>\n",
       "      <td>0.156312</td>\n",
       "      <td>0.169290</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>0.341876</td>\n",
       "      <td>0.057988</td>\n",
       "      <td>0.166393</td>\n",
       "      <td>0.005595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V130</th>\n",
       "      <td>0.010954</td>\n",
       "      <td>0.120154</td>\n",
       "      <td>0.022328</td>\n",
       "      <td>0.115095</td>\n",
       "      <td>0.053549</td>\n",
       "      <td>0.033165</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.029130</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080794</td>\n",
       "      <td>0.233685</td>\n",
       "      <td>0.315459</td>\n",
       "      <td>0.037275</td>\n",
       "      <td>0.065589</td>\n",
       "      <td>0.013139</td>\n",
       "      <td>0.106835</td>\n",
       "      <td>0.020693</td>\n",
       "      <td>0.046001</td>\n",
       "      <td>0.003590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V131</th>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.039181</td>\n",
       "      <td>0.136264</td>\n",
       "      <td>0.009747</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.184182</td>\n",
       "      <td>0.256997</td>\n",
       "      <td>0.144764</td>\n",
       "      <td>0.171113</td>\n",
       "      <td>0.040702</td>\n",
       "      <td>0.326795</td>\n",
       "      <td>0.057241</td>\n",
       "      <td>0.158881</td>\n",
       "      <td>0.005693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V133</th>\n",
       "      <td>0.026090</td>\n",
       "      <td>0.027879</td>\n",
       "      <td>0.045566</td>\n",
       "      <td>0.069431</td>\n",
       "      <td>0.032190</td>\n",
       "      <td>0.070991</td>\n",
       "      <td>0.021402</td>\n",
       "      <td>0.028604</td>\n",
       "      <td>0.008978</td>\n",
       "      <td>0.006843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044967</td>\n",
       "      <td>0.243024</td>\n",
       "      <td>0.320577</td>\n",
       "      <td>0.086146</td>\n",
       "      <td>0.067764</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.057101</td>\n",
       "      <td>0.005146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V147</th>\n",
       "      <td>0.070106</td>\n",
       "      <td>0.083527</td>\n",
       "      <td>0.066623</td>\n",
       "      <td>0.130908</td>\n",
       "      <td>0.143311</td>\n",
       "      <td>0.186524</td>\n",
       "      <td>0.195782</td>\n",
       "      <td>0.188203</td>\n",
       "      <td>0.033622</td>\n",
       "      <td>0.028286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018829</td>\n",
       "      <td>0.179253</td>\n",
       "      <td>0.082228</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.158263</td>\n",
       "      <td>0.127429</td>\n",
       "      <td>0.055410</td>\n",
       "      <td>0.056720</td>\n",
       "      <td>0.030848</td>\n",
       "      <td>0.012437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V155</th>\n",
       "      <td>0.021129</td>\n",
       "      <td>0.049939</td>\n",
       "      <td>0.015498</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.206882</td>\n",
       "      <td>0.630224</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>0.047271</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.007084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>0.032169</td>\n",
       "      <td>0.009837</td>\n",
       "      <td>0.057353</td>\n",
       "      <td>0.084424</td>\n",
       "      <td>0.078694</td>\n",
       "      <td>0.042687</td>\n",
       "      <td>0.644003</td>\n",
       "      <td>0.037647</td>\n",
       "      <td>0.057698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V159</th>\n",
       "      <td>0.022037</td>\n",
       "      <td>0.276736</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.216717</td>\n",
       "      <td>0.252358</td>\n",
       "      <td>0.044692</td>\n",
       "      <td>0.037151</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.018703</td>\n",
       "      <td>0.013731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035530</td>\n",
       "      <td>0.014907</td>\n",
       "      <td>0.019547</td>\n",
       "      <td>0.102609</td>\n",
       "      <td>0.548855</td>\n",
       "      <td>0.152025</td>\n",
       "      <td>0.135569</td>\n",
       "      <td>0.023046</td>\n",
       "      <td>0.337749</td>\n",
       "      <td>0.021354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V161</th>\n",
       "      <td>0.083552</td>\n",
       "      <td>0.094999</td>\n",
       "      <td>0.073403</td>\n",
       "      <td>0.121504</td>\n",
       "      <td>0.163702</td>\n",
       "      <td>0.019040</td>\n",
       "      <td>0.124592</td>\n",
       "      <td>0.087058</td>\n",
       "      <td>0.018544</td>\n",
       "      <td>0.014227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.018420</td>\n",
       "      <td>0.080143</td>\n",
       "      <td>0.256243</td>\n",
       "      <td>0.720387</td>\n",
       "      <td>0.026036</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>0.136103</td>\n",
       "      <td>0.569964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V162</th>\n",
       "      <td>0.020145</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.011021</td>\n",
       "      <td>0.030070</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.032722</td>\n",
       "      <td>0.028217</td>\n",
       "      <td>0.023083</td>\n",
       "      <td>0.149970</td>\n",
       "      <td>0.070993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498194</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.006650</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>0.025088</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>0.042855</td>\n",
       "      <td>0.008563</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.000351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V166</th>\n",
       "      <td>0.047580</td>\n",
       "      <td>0.077935</td>\n",
       "      <td>0.063256</td>\n",
       "      <td>0.135493</td>\n",
       "      <td>0.040099</td>\n",
       "      <td>0.030996</td>\n",
       "      <td>0.070869</td>\n",
       "      <td>0.045699</td>\n",
       "      <td>0.017760</td>\n",
       "      <td>0.043459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318887</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>0.007959</td>\n",
       "      <td>0.181452</td>\n",
       "      <td>0.206607</td>\n",
       "      <td>0.033142</td>\n",
       "      <td>0.386450</td>\n",
       "      <td>0.020072</td>\n",
       "      <td>0.080707</td>\n",
       "      <td>0.006724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V168</th>\n",
       "      <td>0.035978</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>0.032005</td>\n",
       "      <td>0.041013</td>\n",
       "      <td>0.036678</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.045332</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.101132</td>\n",
       "      <td>0.109543</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.059950</td>\n",
       "      <td>0.049859</td>\n",
       "      <td>0.003888</td>\n",
       "      <td>0.101264</td>\n",
       "      <td>0.014114</td>\n",
       "      <td>0.036239</td>\n",
       "      <td>0.004878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V197</th>\n",
       "      <td>0.013728</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>0.020839</td>\n",
       "      <td>0.008921</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.027374</td>\n",
       "      <td>0.010749</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733499</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>0.029872</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>0.016834</td>\n",
       "      <td>0.005745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V198</th>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.017295</td>\n",
       "      <td>0.032319</td>\n",
       "      <td>0.097599</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.733499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.011843</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.012190</td>\n",
       "      <td>0.026940</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V204</th>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.141455</td>\n",
       "      <td>0.053341</td>\n",
       "      <td>0.047651</td>\n",
       "      <td>0.151183</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.170276</td>\n",
       "      <td>0.117034</td>\n",
       "      <td>0.020632</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059950</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.131108</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.351837</td>\n",
       "      <td>0.213187</td>\n",
       "      <td>0.550619</td>\n",
       "      <td>0.047031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V209</th>\n",
       "      <td>0.026723</td>\n",
       "      <td>0.059831</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.183742</td>\n",
       "      <td>0.096099</td>\n",
       "      <td>0.043464</td>\n",
       "      <td>0.176821</td>\n",
       "      <td>0.112182</td>\n",
       "      <td>0.039980</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049859</td>\n",
       "      <td>0.029872</td>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.131108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.483422</td>\n",
       "      <td>0.427664</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>0.205056</td>\n",
       "      <td>0.020652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V211</th>\n",
       "      <td>0.084196</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.067780</td>\n",
       "      <td>0.046070</td>\n",
       "      <td>0.103397</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>0.309185</td>\n",
       "      <td>0.187271</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003888</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.011843</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.483422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101817</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>0.135230</td>\n",
       "      <td>0.450467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V212</th>\n",
       "      <td>0.052363</td>\n",
       "      <td>0.202903</td>\n",
       "      <td>0.091373</td>\n",
       "      <td>0.074172</td>\n",
       "      <td>0.033966</td>\n",
       "      <td>0.053557</td>\n",
       "      <td>0.066612</td>\n",
       "      <td>0.086840</td>\n",
       "      <td>0.031857</td>\n",
       "      <td>0.045686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101264</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.351837</td>\n",
       "      <td>0.427664</td>\n",
       "      <td>0.101817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.107369</td>\n",
       "      <td>0.022132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V213</th>\n",
       "      <td>0.076290</td>\n",
       "      <td>0.011735</td>\n",
       "      <td>0.048549</td>\n",
       "      <td>0.048482</td>\n",
       "      <td>0.199822</td>\n",
       "      <td>0.388307</td>\n",
       "      <td>0.100480</td>\n",
       "      <td>0.070597</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.013939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014114</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>0.012190</td>\n",
       "      <td>0.213187</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208010</td>\n",
       "      <td>0.080118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V217</th>\n",
       "      <td>0.169221</td>\n",
       "      <td>0.199041</td>\n",
       "      <td>0.052502</td>\n",
       "      <td>0.247755</td>\n",
       "      <td>0.034295</td>\n",
       "      <td>0.018967</td>\n",
       "      <td>0.273925</td>\n",
       "      <td>0.068254</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036239</td>\n",
       "      <td>0.016834</td>\n",
       "      <td>0.026940</td>\n",
       "      <td>0.550619</td>\n",
       "      <td>0.205056</td>\n",
       "      <td>0.135230</td>\n",
       "      <td>0.107369</td>\n",
       "      <td>0.208010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V219</th>\n",
       "      <td>0.021924</td>\n",
       "      <td>0.003845</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.068028</td>\n",
       "      <td>0.033442</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>0.022176</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.047031</td>\n",
       "      <td>0.020652</td>\n",
       "      <td>0.450467</td>\n",
       "      <td>0.022132</td>\n",
       "      <td>0.080118</td>\n",
       "      <td>0.027440</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            V1        V2        V3       V29       V33       V57       V64  \\\n",
       "V1    1.000000  0.164772  0.525073  0.078567  0.060589  0.021213  0.588236   \n",
       "V2    0.164772  1.000000  0.256672  0.252714  0.714931  0.025463  0.162511   \n",
       "V3    0.525073  0.256672  1.000000  0.031332  0.111155  0.001457  0.420424   \n",
       "V29   0.078567  0.252714  0.031332  1.000000  0.395709  0.267549  0.092134   \n",
       "V33   0.060589  0.714931  0.111155  0.395709  1.000000  0.254586  0.185031   \n",
       "V57   0.021213  0.025463  0.001457  0.267549  0.254586  1.000000  0.033783   \n",
       "V64   0.588236  0.162511  0.420424  0.092134  0.185031  0.033783  1.000000   \n",
       "V66   0.243176  0.346018  0.532158  0.060104  0.439326  0.036850  0.633490   \n",
       "V71   0.005165  0.022192  0.004056  0.045258  0.018390  0.056106  0.016414   \n",
       "V72   0.005494  0.015374  0.003998  0.048409  0.004348  0.057690  0.007736   \n",
       "V75   0.023095  0.075923  0.042176  0.009927  0.017992  0.039186  0.012021   \n",
       "V77   0.023637  0.001137  0.018817  0.021278  0.060634  0.018548  0.038750   \n",
       "V78   0.020282  0.012507  0.013327  0.031384  0.003521  0.004248  0.019306   \n",
       "V82   0.004264  0.148171  0.036439  0.318393  0.119526  0.061292  0.004502   \n",
       "V84   0.029294  0.013868  0.023310  0.035791  0.036242  0.023708  0.047744   \n",
       "V85   0.016823  0.006358  0.013461  0.028996  0.000205  0.083893  0.025851   \n",
       "V86   0.000472  0.019905  0.003832  0.026897  0.029842  0.093244  0.000625   \n",
       "V92   0.007852  0.009991  0.016816  0.288594  0.050848  0.143639  0.000088   \n",
       "V93   0.018843  0.575762  0.072405  0.401571  0.693993  0.083188  0.134149   \n",
       "V94   0.000438  0.087225  0.001331  0.301320  0.048457  0.067697  0.012975   \n",
       "V95   0.023250  0.528391  0.054079  0.059608  0.546288  0.019230  0.103598   \n",
       "V98   0.015292  0.399063  0.028466  0.187630  0.407637  0.035857  0.078927   \n",
       "V99   0.005188  0.022274  0.000238  0.215299  0.027630  0.137521  0.009413   \n",
       "V100  0.036947  0.569314  0.076910  0.308130  0.624120  0.080993  0.115139   \n",
       "V102  0.031435  0.519671  0.051189  0.055283  0.474846  0.029650  0.070052   \n",
       "V105  0.007914  0.413909  0.000394  0.231107  0.416898  0.044401  0.020652   \n",
       "V106  0.006670  0.011513  0.006070  0.110009  0.040599  0.095905  0.009169   \n",
       "V108  0.004410  0.155660  0.005147  0.271191  0.094728  0.076744  0.009786   \n",
       "V109  0.011598  0.503818  0.053433  0.092785  0.602260  0.021091  0.102038   \n",
       "V112  0.014384  0.438434  0.017430  0.285606  0.532213  0.013713  0.072018   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "V114  0.055756  0.578957  0.108840  0.355018  0.771736  0.097048  0.153839   \n",
       "V116  0.021234  0.431458  0.054499  0.013889  0.497771  0.014346  0.093128   \n",
       "V119  0.009473  0.453095  0.009112  0.290224  0.556250  0.020977  0.070666   \n",
       "V120  0.024479  0.123279  0.035893  0.102624  0.218547  0.224462  0.117536   \n",
       "V121  0.043232  0.115313  0.016108  0.030549  0.155389  0.109730  0.032174   \n",
       "V122  0.039066  0.034196  0.001038  0.132005  0.061508  0.045776  0.183446   \n",
       "V123  0.037099  0.265170  0.072702  0.062577  0.304482  0.146078  0.179327   \n",
       "V124  0.048836  0.129674  0.041528  0.183441  0.136754  0.054171  0.273243   \n",
       "V126  0.035973  0.028308  0.029669  0.190180  0.076719  0.158620  0.095549   \n",
       "V127  0.014733  0.096063  0.000640  0.023733  0.083801  0.058743  0.002637   \n",
       "V129  0.009710  0.012062  0.031492  0.068995  0.038607  0.003859  0.001098   \n",
       "V130  0.010954  0.120154  0.022328  0.115095  0.053549  0.033165  0.005709   \n",
       "V131  0.010763  0.064362  0.039181  0.136264  0.009747  0.008657  0.001686   \n",
       "V133  0.026090  0.027879  0.045566  0.069431  0.032190  0.070991  0.021402   \n",
       "V147  0.070106  0.083527  0.066623  0.130908  0.143311  0.186524  0.195782   \n",
       "V155  0.021129  0.049939  0.015498  0.006736  0.206882  0.630224  0.018722   \n",
       "V159  0.022037  0.276736  0.013420  0.216717  0.252358  0.044692  0.037151   \n",
       "V161  0.083552  0.094999  0.073403  0.121504  0.163702  0.019040  0.124592   \n",
       "V162  0.020145  0.001650  0.011021  0.030070  0.001698  0.032722  0.028217   \n",
       "V166  0.047580  0.077935  0.063256  0.135493  0.040099  0.030996  0.070869   \n",
       "V168  0.035978  0.003934  0.032005  0.041013  0.036678  0.039835  0.045332   \n",
       "V197  0.013728  0.002111  0.007239  0.020839  0.008921  0.098333  0.018541   \n",
       "V198  0.002457  0.009535  0.000467  0.017295  0.032319  0.097599  0.000031   \n",
       "V204  0.014086  0.141455  0.053341  0.047651  0.151183  0.008322  0.170276   \n",
       "V209  0.026723  0.059831  0.004800  0.183742  0.096099  0.043464  0.176821   \n",
       "V211  0.084196  0.000694  0.067780  0.046070  0.103397  0.012034  0.309185   \n",
       "V212  0.052363  0.202903  0.091373  0.074172  0.033966  0.053557  0.066612   \n",
       "V213  0.076290  0.011735  0.048549  0.048482  0.199822  0.388307  0.100480   \n",
       "V217  0.169221  0.199041  0.052502  0.247755  0.034295  0.018967  0.273925   \n",
       "V219  0.021924  0.003845  0.067797  0.068028  0.033442  0.021013  0.013924   \n",
       "\n",
       "           V66       V71       V72    ...         V168      V197      V198  \\\n",
       "V1    0.243176  0.005165  0.005494    ...     0.035978  0.013728  0.002457   \n",
       "V2    0.346018  0.022192  0.015374    ...     0.003934  0.002111  0.009535   \n",
       "V3    0.532158  0.004056  0.003998    ...     0.032005  0.007239  0.000467   \n",
       "V29   0.060104  0.045258  0.048409    ...     0.041013  0.020839  0.017295   \n",
       "V33   0.439326  0.018390  0.004348    ...     0.036678  0.008921  0.032319   \n",
       "V57   0.036850  0.056106  0.057690    ...     0.039835  0.098333  0.097599   \n",
       "V64   0.633490  0.016414  0.007736    ...     0.045332  0.018541  0.000031   \n",
       "V66   1.000000  0.000215  0.006867    ...     0.002210  0.027374  0.019202   \n",
       "V71   0.000215  1.000000  0.724406    ...     0.101132  0.010749  0.010854   \n",
       "V72   0.006867  0.724406  1.000000    ...     0.109543  0.005160  0.003558   \n",
       "V75   0.037279  0.058321  0.019759    ...     0.064623  0.031676  0.011425   \n",
       "V77   0.019779  0.577050  0.054681    ...     0.293418  0.005820  0.007512   \n",
       "V78   0.005603  0.385538  0.252957    ...     0.005174  0.002044  0.001966   \n",
       "V82   0.079028  0.002469  0.000476    ...     0.022068  0.024441  0.013781   \n",
       "V84   0.024668  0.255923  0.069510    ...     0.024003  0.003580  0.003014   \n",
       "V85   0.025404  0.010634  0.001123    ...     0.001877  0.775048  0.354298   \n",
       "V86   0.014879  0.014608  0.008399    ...     0.001433  0.461420  0.662646   \n",
       "V92   0.001943  0.116916  0.098512    ...     0.020332  0.025295  0.033678   \n",
       "V93   0.374343  0.067785  0.136332    ...     0.036538  0.007442  0.022605   \n",
       "V94   0.037319  0.020918  0.018154    ...     0.018166  0.004200  0.004114   \n",
       "V95   0.306761  0.015665  0.058793    ...     0.015530  0.010786  0.011081   \n",
       "V98   0.208071  0.019319  0.033173    ...     0.009764  0.000789  0.002552   \n",
       "V99   0.027522  0.174382  0.145782    ...     0.006211  0.024117  0.031120   \n",
       "V100  0.315364  0.058269  0.105753    ...     0.046823  0.009927  0.018696   \n",
       "V102  0.235063  0.017144  0.051797    ...     0.027563  0.012152  0.009723   \n",
       "V105  0.169926  0.033811  0.015883    ...     0.076633  0.001811  0.007346   \n",
       "V106  0.000550  0.032458  0.025747    ...     0.015560  0.021121  0.017707   \n",
       "V108  0.063870  0.003788  0.004983    ...     0.021889  0.005116  0.009960   \n",
       "V109  0.283858  0.028431  0.071750    ...     0.019684  0.018089  0.020947   \n",
       "V112  0.208409  0.041159  0.039403    ...     0.045597  0.000796  0.012229   \n",
       "...        ...       ...       ...    ...          ...       ...       ...   \n",
       "V114  0.355882  0.084732  0.139246    ...     0.017420  0.041035  0.051812   \n",
       "V116  0.253894  0.027339  0.081025    ...     0.014763  0.019454  0.020440   \n",
       "V119  0.206058  0.049685  0.025991    ...     0.059429  0.002940  0.009730   \n",
       "V120  0.168481  0.042805  0.024601    ...     0.000904  0.330656  0.288206   \n",
       "V121  0.069082  0.031589  0.004237    ...     0.010394  0.238220  0.305404   \n",
       "V122  0.075541  0.012944  0.004840    ...     0.022755  0.059850  0.097070   \n",
       "V123  0.261780  0.042441  0.023787    ...     0.019876  0.305268  0.290885   \n",
       "V124  0.231945  0.019057  0.020687    ...     0.038141  0.218596  0.170365   \n",
       "V126  0.113448  0.012374  0.015503    ...     0.033694  0.280281  0.151684   \n",
       "V127  0.028779  0.002872  0.024965    ...     0.053291  0.216594  0.282699   \n",
       "V129  0.010713  0.008017  0.012187    ...     0.017581  0.010661  0.025816   \n",
       "V130  0.029130  0.018115  0.002311    ...     0.080794  0.233685  0.315459   \n",
       "V131  0.003274  0.015827  0.010453    ...     0.042481  0.184182  0.256997   \n",
       "V133  0.028604  0.008978  0.006843    ...     0.044967  0.243024  0.320577   \n",
       "V147  0.188203  0.033622  0.028286    ...     0.018829  0.179253  0.082228   \n",
       "V155  0.047271  0.001323  0.007084    ...     0.005321  0.032169  0.009837   \n",
       "V159  0.043000  0.018703  0.013731    ...     0.035530  0.014907  0.019547   \n",
       "V161  0.087058  0.018544  0.014227    ...     0.011924  0.014874  0.018420   \n",
       "V162  0.023083  0.149970  0.070993    ...     0.498194  0.005259  0.006650   \n",
       "V166  0.045699  0.017760  0.043459    ...     0.318887  0.011250  0.007959   \n",
       "V168  0.002210  0.101132  0.109543    ...     1.000000  0.001329  0.000704   \n",
       "V197  0.027374  0.010749  0.005160    ...     0.001329  1.000000  0.733499   \n",
       "V198  0.019202  0.010854  0.003558    ...     0.000704  0.733499  1.000000   \n",
       "V204  0.117034  0.020632  0.000559    ...     0.059950  0.019474  0.032304   \n",
       "V209  0.112182  0.039980  0.010683    ...     0.049859  0.029872  0.028797   \n",
       "V211  0.187271  0.004999  0.000920    ...     0.003888  0.007017  0.011843   \n",
       "V212  0.086840  0.031857  0.045686    ...     0.101264  0.000730  0.027009   \n",
       "V213  0.070597  0.004954  0.013939    ...     0.014114  0.030884  0.012190   \n",
       "V217  0.068254  0.006837  0.001486    ...     0.036239  0.016834  0.026940   \n",
       "V219  0.005531  0.022176  0.003168    ...     0.004878  0.005745  0.000562   \n",
       "\n",
       "          V204      V209      V211      V212      V213      V217      V219  \n",
       "V1    0.014086  0.026723  0.084196  0.052363  0.076290  0.169221  0.021924  \n",
       "V2    0.141455  0.059831  0.000694  0.202903  0.011735  0.199041  0.003845  \n",
       "V3    0.053341  0.004800  0.067780  0.091373  0.048549  0.052502  0.067797  \n",
       "V29   0.047651  0.183742  0.046070  0.074172  0.048482  0.247755  0.068028  \n",
       "V33   0.151183  0.096099  0.103397  0.033966  0.199822  0.034295  0.033442  \n",
       "V57   0.008322  0.043464  0.012034  0.053557  0.388307  0.018967  0.021013  \n",
       "V64   0.170276  0.176821  0.309185  0.066612  0.100480  0.273925  0.013924  \n",
       "V66   0.117034  0.112182  0.187271  0.086840  0.070597  0.068254  0.005531  \n",
       "V71   0.020632  0.039980  0.004999  0.031857  0.004954  0.006837  0.022176  \n",
       "V72   0.000559  0.010683  0.000920  0.045686  0.013939  0.001486  0.003168  \n",
       "V75   0.092412  0.068151  0.004465  0.072529  0.015356  0.073781  0.013252  \n",
       "V77   0.043695  0.031890  0.006947  0.012540  0.029732  0.028865  0.038583  \n",
       "V78   0.025601  0.015200  0.010073  0.016404  0.002894  0.005441  0.002567  \n",
       "V82   0.002306  0.029772  0.025725  0.139273  0.011638  0.113540  0.009942  \n",
       "V84   0.006329  0.029089  0.013675  0.008222  0.011632  0.005266  0.028037  \n",
       "V85   0.005134  0.045251  0.010923  0.029905  0.034153  0.014977  0.006764  \n",
       "V86   0.024028  0.065741  0.015831  0.004793  0.013557  0.027293  0.000906  \n",
       "V92   0.050302  0.108412  0.073215  0.077663  0.054456  0.062071  0.018570  \n",
       "V93   0.122997  0.032451  0.064693  0.142165  0.029419  0.000851  0.059523  \n",
       "V94   0.011274  0.024640  0.083110  0.008746  0.052883  0.115318  0.131930  \n",
       "V95   0.072355  0.013633  0.020417  0.089509  0.052611  0.007191  0.007597  \n",
       "V98   0.069142  0.007192  0.019643  0.106789  0.019617  0.026385  0.002817  \n",
       "V99   0.043814  0.093463  0.040244  0.059480  0.041967  0.043075  0.009439  \n",
       "V100  0.129573  0.026293  0.045778  0.193899  0.040689  0.005088  0.065743  \n",
       "V102  0.116441  0.070824  0.001930  0.182974  0.052991  0.019441  0.007429  \n",
       "V105  0.113695  0.050977  0.000148  0.183647  0.026750  0.011816  0.019131  \n",
       "V106  0.036817  0.057050  0.020206  0.031203  0.019541  0.035629  0.015729  \n",
       "V108  0.024975  0.044948  0.070680  0.020478  0.065618  0.101455  0.125820  \n",
       "V109  0.090694  0.032433  0.013345  0.051816  0.057092  0.032216  0.008461  \n",
       "V112  0.100512  0.021476  0.011796  0.101349  0.022670  0.013104  0.028031  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "V114  0.091002  0.079052  0.077040  0.061979  0.013143  0.024999  0.083265  \n",
       "V116  0.104428  0.022399  0.014718  0.074737  0.061194  0.005083  0.025065  \n",
       "V119  0.103174  0.023239  0.021707  0.087009  0.033779  0.009981  0.051464  \n",
       "V120  0.118536  0.034621  0.031875  0.000909  0.158258  0.056918  0.004132  \n",
       "V121  0.215628  0.135390  0.107591  0.100124  0.150176  0.062184  0.004020  \n",
       "V122  0.064537  0.174386  0.250981  0.024876  0.052128  0.008842  0.001032  \n",
       "V123  0.220502  0.024632  0.056134  0.115467  0.200473  0.133672  0.009966  \n",
       "V124  0.118602  0.120661  0.256952  0.068526  0.102666  0.106746  0.008345  \n",
       "V126  0.081406  0.166708  0.067934  0.129529  0.006328  0.036173  0.022472  \n",
       "V127  0.096500  0.085018  0.017383  0.177310  0.026371  0.098456  0.001871  \n",
       "V129  0.156312  0.169290  0.041893  0.341876  0.057988  0.166393  0.005595  \n",
       "V130  0.037275  0.065589  0.013139  0.106835  0.020693  0.046001  0.003590  \n",
       "V131  0.144764  0.171113  0.040702  0.326795  0.057241  0.158881  0.005693  \n",
       "V133  0.086146  0.067764  0.003813  0.126500  0.014050  0.057101  0.005146  \n",
       "V147  0.007308  0.158263  0.127429  0.055410  0.056720  0.030848  0.012437  \n",
       "V155  0.057353  0.084424  0.078694  0.042687  0.644003  0.037647  0.057698  \n",
       "V159  0.102609  0.548855  0.152025  0.135569  0.023046  0.337749  0.021354  \n",
       "V161  0.080143  0.256243  0.720387  0.026036  0.009147  0.136103  0.569964  \n",
       "V162  0.024283  0.025088  0.004719  0.042855  0.008563  0.003969  0.000351  \n",
       "V166  0.181452  0.206607  0.033142  0.386450  0.020072  0.080707  0.006724  \n",
       "V168  0.059950  0.049859  0.003888  0.101264  0.014114  0.036239  0.004878  \n",
       "V197  0.019474  0.029872  0.007017  0.000730  0.030884  0.016834  0.005745  \n",
       "V198  0.032304  0.028797  0.011843  0.027009  0.012190  0.026940  0.000562  \n",
       "V204  1.000000  0.131108  0.079245  0.351837  0.213187  0.550619  0.047031  \n",
       "V209  0.131108  1.000000  0.483422  0.427664  0.007834  0.205056  0.020652  \n",
       "V211  0.079245  0.483422  1.000000  0.101817  0.008710  0.135230  0.450467  \n",
       "V212  0.351837  0.427664  0.101817  1.000000  0.022355  0.107369  0.022132  \n",
       "V213  0.213187  0.007834  0.008710  0.022355  1.000000  0.208010  0.080118  \n",
       "V217  0.550619  0.205056  0.135230  0.107369  0.208010  1.000000  0.027440  \n",
       "V219  0.047031  0.020652  0.450467  0.022132  0.080118  0.027440  1.000000  \n",
       "\n",
       "[61 rows x 61 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifying again if any correlation exist\n",
    "\n",
    "dropped_df.corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the same highly correlated variable from Test Data\n",
    "\n",
    "X_test = X_test.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['V1', 'V2', 'V3', 'V29', 'V33', 'V57', 'V64', 'V66', 'V71', 'V72',\n",
      "       'V75', 'V77', 'V78', 'V82', 'V84', 'V85', 'V86', 'V92', 'V93', 'V94',\n",
      "       'V95', 'V98', 'V99', 'V100', 'V102', 'V105', 'V106', 'V108', 'V109',\n",
      "       'V112', 'V113', 'V114', 'V116', 'V119', 'V120', 'V121', 'V122', 'V123',\n",
      "       'V124', 'V126', 'V127', 'V129', 'V130', 'V131', 'V133', 'V147', 'V155',\n",
      "       'V159', 'V161', 'V162', 'V166', 'V168', 'V197', 'V198', 'V204', 'V209',\n",
      "       'V211', 'V212', 'V213', 'V217', 'V219'],\n",
      "      dtype='object')\n",
      "(400, 61)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.columns)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(926, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Undersampling the data to balance\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "ros = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_sample(dropped_df,Y)\n",
    "print(X_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardizing numerical variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_validation = sc.transform(X_validation) \n",
    "X_test = sc.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train count ****  Counter({'Bad': 375, 'Good': 365})\n",
      "validation count ****  Counter({'Good': 98, 'Bad': 88})\n"
     ]
    }
   ],
   "source": [
    "#### Check the data distribution of train and validation target variable\n",
    "\n",
    "import collections\n",
    "train_count = collections.Counter(y_train)\n",
    "validation_count = collections.Counter(y_validation)\n",
    "print(\"train count **** \",train_count )\n",
    "print(\"validation count **** \",validation_count )\n",
    "#collections.Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print TP,TN,FP,FN and calculate the total cost bear by manufacturing company on misclassifyng the device\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#Positive is Bad , Negative is Good\n",
    "def print_confusion_matrix(true, pred):\n",
    "    cm = confusion_matrix(true, pred)\n",
    "    print('True positive = ', cm[0][0])\n",
    "    print('False positive = ', cm[1][0])\n",
    "    print('False negative = ', cm[0][1])\n",
    "    print('True negative = ', cm[1][1])\n",
    "    Total_Cost = (cost_FN * cm[0][1]) + (cost_FP * cm[1][0])\n",
    "    print(\"Total Cost \", Total_Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training model using Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 1.00\n",
      "[[335  40]\n",
      " [ 90 275]]\n",
      "True positive =  335\n",
      "False positive =  90\n",
      "False negative =  40\n",
      "True negative =  275\n",
      "Total Cost  245000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.79      0.89      0.84       375\n",
      "       Good       0.87      0.75      0.81       365\n",
      "\n",
      "avg / total       0.83      0.82      0.82       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Predicton and measure on training data for logistic model\n",
    "\n",
    "\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_train, y_train_pred)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.79\n",
      "[[77 11]\n",
      " [28 70]]\n",
      "True positive =  77\n",
      "False positive =  28\n",
      "False negative =  11\n",
      "True negative =  70\n",
      "Total Cost  69000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.73      0.88      0.80        88\n",
      "       Good       0.86      0.71      0.78        98\n",
      "\n",
      "avg / total       0.80      0.79      0.79       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on Validation data for logistic model\n",
    "\n",
    "y_pred = logreg.predict(X_validation)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_validation, y_validation)))\n",
    "#print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred)))\n",
    "#Confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good    254\n",
      "Bad     146\n",
      "Name: Machine_State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#test predictions for logistic model\n",
    "\n",
    "test_logistic_pred = logreg.predict(X_test)\n",
    "pred_test_logistic = pd.DataFrame(test_logistic_pred, columns=['Machine_State'])\n",
    "print(pd.value_counts(pred_test_logistic['Machine_State']))\n",
    "\n",
    "#submission = pd.concat([SerialNum, pred_test_logistic], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.68\n"
     ]
    }
   ],
   "source": [
    "######################Gaussian Model\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = GaussianNB()\n",
    "naive = model.fit(X_train, y_train)\n",
    "naive_preds = naive.predict(X_validation)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, naive_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.64\n",
      "[[369   6]\n",
      " [257 108]]\n",
      "True positive =  369\n",
      "False positive =  257\n",
      "False negative =  6\n",
      "True negative =  108\n",
      "Total Cost  158500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.59      0.98      0.74       375\n",
      "       Good       0.95      0.30      0.45       365\n",
      "\n",
      "avg / total       0.77      0.64      0.60       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for Naive Bayes model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_naive = naive.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_naive)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_naive,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_naive)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.68\n",
      "[[87  1]\n",
      " [58 40]]\n",
      "True positive =  87\n",
      "False positive =  58\n",
      "False negative =  1\n",
      "True negative =  40\n",
      "Total Cost  34000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.60      0.99      0.75        88\n",
      "       Good       0.98      0.41      0.58        98\n",
      "\n",
      "avg / total       0.80      0.68      0.66       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for Naive Bayes model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation = naive.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good    400\n",
      "Name: Machine_State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#test predictions on naive Bayes model\n",
    "\n",
    "test_naive_pred = naive.predict(X_test)\n",
    "pred_test_naive = pd.DataFrame(test_naive_pred, columns=['Machine_State'])\n",
    "print(pd.value_counts(pred_test_naive['Machine_State']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_split=2)\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1.00\n",
      "[[375   0]\n",
      " [  0 365]]\n",
      "True positive =  375\n",
      "False positive =  0\n",
      "False negative =  0\n",
      "True negative =  365\n",
      "Total Cost  0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       1.00      1.00      1.00       375\n",
      "       Good       1.00      1.00      1.00       365\n",
      "\n",
      "avg / total       1.00      1.00      1.00       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for Random forest model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_rf)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_rf,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_rf)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.86\n",
      "[[83  5]\n",
      " [21 77]]\n",
      "True positive =  83\n",
      "False positive =  21\n",
      "False negative =  5\n",
      "True negative =  77\n",
      "Total Cost  35500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.80      0.94      0.86        88\n",
      "       Good       0.94      0.79      0.86        98\n",
      "\n",
      "avg / total       0.87      0.86      0.86       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for Random forest model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_rf = rf_model.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_rf)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_rf,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_rf)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good    397\n",
      "Bad       3\n",
      "Name: Machine_State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#test predictions usinfRandomForest model\n",
    "\n",
    "test_rf_pred = rf.predict(X_test)\n",
    "pred_test_rf = pd.DataFrame(test_rf_pred, columns=['Machine_State'])\n",
    "print(pd.value_counts(pred_test_rf['Machine_State']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier using rbf on training set: 1.00\n",
      "Accuracy of SVM classifier using rbf on validation set: 0.68\n",
      "Accuracy of SVM classifier using kernel=linear on training set: 0.83\n",
      "Accuracy of SVM classifier using kernel=linear on validation set: 0.79\n",
      "Accuracy of SVM classifier using LinearSVC on training set: 0.83\n",
      "Accuracy of SVM classifier using LinearSVC on validation set: 0.78\n",
      "Accuracy of SVM classifier using kernel=poly on training set: 0.70\n",
      "Accuracy of SVM classifier using kernel=poly on validation set: 0.63\n"
     ]
    }
   ],
   "source": [
    "#### SVM \n",
    "from sklearn import svm    \t\t\t# To fit the svm classifier\n",
    "\n",
    "C = 1.0  # SVM regularization parameter\n",
    "# SVC with RBF kernel\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X_train, y_train)\n",
    "# SVC with linear kernel\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X_train, y_train)\n",
    "# LinearSVC (linear kernel)\n",
    "lin_svc = svm.LinearSVC(C=C).fit(X_train, y_train)\n",
    "# SVC with polynomial (degree 3) kernel\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X_train, y_train)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "print('Accuracy of SVM classifier using rbf on training set: {:.2f}'\n",
    "     .format(rbf_svc.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier using rbf on validation set: {:.2f}'\n",
    "     .format(rbf_svc.score(X_validation, y_validation)))\n",
    "print('Accuracy of SVM classifier using kernel=linear on training set: {:.2f}'\n",
    "     .format(svc.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier using kernel=linear on validation set: {:.2f}'\n",
    "     .format(svc.score(X_validation, y_validation)))\n",
    "print('Accuracy of SVM classifier using LinearSVC on training set: {:.2f}'\n",
    "     .format(lin_svc.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier using LinearSVC on validation set: {:.2f}'\n",
    "     .format(lin_svc.score(X_validation, y_validation)))\n",
    "print('Accuracy of SVM classifier using kernel=poly on training set: {:.2f}'\n",
    "     .format(poly_svc.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier using kernel=poly on validation set: {:.2f}'\n",
    "     .format(poly_svc.score(X_validation, y_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters\n",
      "\n",
      "# Tuning hyper-parameters\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Set the parameters by cross-validation\n",
    "parameters = [{'kernel': ['rbf'],\n",
    "               'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5],\n",
    "                'C': [1, 10, 100, 1000]},\n",
    "              {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "print(\"# Tuning hyper-parameters\")\n",
    "print()\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(decision_function_shape='ovr'), parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier using rbf_svc_grid on training set: 0.98\n",
      "Accuracy of SVM classifier using rbf_svc_grid on validation set: 0.78\n"
     ]
    }
   ],
   "source": [
    "# SVC with RBF kernel with best param\n",
    "rbf_svc_grid = svm.SVC(kernel='rbf', gamma=0.01, C=100).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of SVM classifier using rbf_svc_grid on training set: {:.2f}'\n",
    "     .format(rbf_svc_grid.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier using rbf_svc_grid on validation set: {:.2f}'\n",
    "     .format(rbf_svc_grid.score(X_validation, y_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.98\n",
      "[[374   1]\n",
      " [ 15 350]]\n",
      "True positive =  374\n",
      "False positive =  15\n",
      "False negative =  1\n",
      "True negative =  350\n",
      "Total Cost  12500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.96      1.00      0.98       375\n",
      "       Good       1.00      0.96      0.98       365\n",
      "\n",
      "avg / total       0.98      0.98      0.98       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for svm model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_svm = rbf_svc_grid.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_svm)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_svm,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_svm)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.78\n",
      "[[75 13]\n",
      " [27 71]]\n",
      "True positive =  75\n",
      "False positive =  27\n",
      "False negative =  13\n",
      "True negative =  71\n",
      "Total Cost  78500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.74      0.85      0.79        88\n",
      "       Good       0.85      0.72      0.78        98\n",
      "\n",
      "avg / total       0.79      0.78      0.78       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for svm model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_svm = rbf_svc_grid.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_svm)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_svm,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_svm)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good    273\n",
      "Bad     127\n",
      "Name: Machine_State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##test predictions using svm model\n",
    "\n",
    "test_svm_pred = rbf_svc_grid.predict(X_test)\n",
    "pred_test_svm = pd.DataFrame(test_svm_pred, columns=['Machine_State'])\n",
    "print(pd.value_counts(pred_test_svm['Machine_State']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DecisionTree Classifier\n",
    "\n",
    "from sklearn import tree\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "DT_model = DT.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1.00\n",
      "[[375   0]\n",
      " [  0 365]]\n",
      "True positive =  375\n",
      "False positive =  0\n",
      "False negative =  0\n",
      "True negative =  365\n",
      "Total Cost  0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       1.00      1.00      1.00       375\n",
      "       Good       1.00      1.00      1.00       365\n",
      "\n",
      "avg / total       1.00      1.00      1.00       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for Decision Tree model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_DT = DT_model.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_DT)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_DT,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_DT)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.75\n",
      "[[66 22]\n",
      " [25 73]]\n",
      "True positive =  66\n",
      "False positive =  25\n",
      "False negative =  22\n",
      "True negative =  73\n",
      "Total Cost  122500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.73      0.75      0.74        88\n",
      "       Good       0.77      0.74      0.76        98\n",
      "\n",
      "avg / total       0.75      0.75      0.75       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for decision tree  model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_DT  = DT_model.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_DT)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_DT,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_DT)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good    292\n",
      "Bad     108\n",
      "Name: Machine_State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#test predictions using decision tree model\n",
    "\n",
    "test_DT_pred = DT_model.predict(X_test)\n",
    "pred_test_DT = pd.DataFrame(test_DT_pred, columns=['Machine_State'])\n",
    "print(pd.value_counts(pred_test_DT['Machine_State']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = [{\n",
    "#                  'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400],\n",
    "#                  'max_depth': [1, 3, 5, 7, 9, 11],\n",
    "#                 'gamma':[i/10.0 for i in range(0,5)],\n",
    "#                 'subsample':[i/10.0 for i in range(6,10)]\n",
    "# #                 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "#              }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 960 candidates, totalling 4800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 24.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4800 out of 4800 | elapsed: 30.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'gamma': 0.0, 'max_depth': 7, 'n_estimators': 150, 'subsample': 0.6}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_estimators = range(50, 400, 50)\n",
    "max_depth = range(1, 11, 2)\n",
    "param_grid1 = dict(n_estimators=n_estimators,max_depth=max_depth)\n",
    "#kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(xgb, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=5, verbose=1)\n",
    "#grid_search.fit(X, label_encoded_y)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(grid_search.best_params_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.01,\n",
    " n_estimators=150,\n",
    " max_depth=7,\n",
    " min_child_weight=6,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train is 0.91\n",
      "[[356  19]\n",
      " [ 46 319]]\n",
      "True positive =  356\n",
      "False positive =  46\n",
      "False negative =  19\n",
      "True negative =  319\n",
      "Total Cost  118000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.89      0.95      0.92       375\n",
      "       Good       0.94      0.87      0.91       365\n",
      "\n",
      "avg / total       0.91      0.91      0.91       740\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farha\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for xgboost model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "print(\"Accuracy on train is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_xgb)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_xgb,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_xgb)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation is 0.83\n",
      "[[79  9]\n",
      " [22 76]]\n",
      "True positive =  79\n",
      "False positive =  22\n",
      "False negative =  9\n",
      "True negative =  76\n",
      "Total Cost  56000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.78      0.90      0.84        88\n",
      "       Good       0.89      0.78      0.83        98\n",
      "\n",
      "avg / total       0.84      0.83      0.83       186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farha\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for xgboost  model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_xgb  = xgb_model.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy on validation is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_xgb)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_xgb,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_xgb)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = [{\n",
    "#                  'n_estimators': [5, 10, 15, 20, 50],\n",
    "#                  'max_depth': [2, 5, 7, 9],\n",
    "#                 'gamma':[i/10.0 for i in range(0,5)]\n",
    "# #                 'n_jobs' : [-1, 1]\n",
    "#              }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_boost = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=1, max_features='auto').fit(X_train, y_train)\n",
    "#grid_boost = GridSearchCV(grad_boost, param_grid, cv=5)\n",
    "#grid_model = grid_boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train is 0.85\n",
      "[[352  23]\n",
      " [ 86 279]]\n",
      "True positive =  352\n",
      "False positive =  86\n",
      "False negative =  23\n",
      "True negative =  279\n",
      "Total Cost  158000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.80      0.94      0.87       375\n",
      "       Good       0.92      0.76      0.84       365\n",
      "\n",
      "avg / total       0.86      0.85      0.85       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for gbc model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_gb = grad_boost.predict(X_train)\n",
    "print(\"Accuracy on train is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_gb)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_gb,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_gb)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation is 0.79\n",
      "[[82  6]\n",
      " [33 65]]\n",
      "True positive =  82\n",
      "False positive =  33\n",
      "False negative =  6\n",
      "True negative =  65\n",
      "Total Cost  46500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.71      0.93      0.81        88\n",
      "       Good       0.92      0.66      0.77        98\n",
      "\n",
      "avg / total       0.82      0.79      0.79       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for gradientboosting  model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_gb  = grad_boost.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy on validation is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_gb)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_gb,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_gb)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 10,\n",
      " 'n_jobs': 1,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_grid = RandomForestClassifier(random_state = 42)\n",
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf_grid.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf= RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 11.2min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the random search model\n",
    "rf_fit = rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': 80,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 1400}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation is 0.86\n",
      "[[79  9]\n",
      " [17 81]]\n",
      "True positive =  79\n",
      "False positive =  17\n",
      "False negative =  9\n",
      "True negative =  81\n",
      "Total Cost  53500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.82      0.90      0.86        88\n",
      "       Good       0.90      0.83      0.86        98\n",
      "\n",
      "avg / total       0.86      0.86      0.86       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for gradientboosting  model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_rfgrid = rf_fit.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy on validation is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_rfgrid)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_rfgrid,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_rfgrid)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_rfgrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model3 = RandomForestClassifier(n_estimators=1400, criterion='entropy', min_samples_split=5,bootstrap= False, max_depth = 80,max_features ='auto' ,min_samples_leaf = 1)\n",
    "rf_modl = rf_model3.fit(X_train, y_train)\n",
    "rf_pred3 = rf_modl.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1.00\n",
      "[[375   0]\n",
      " [  0 365]]\n",
      "True positive =  375\n",
      "False positive =  0\n",
      "False negative =  0\n",
      "True negative =  365\n",
      "Total Cost  0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       1.00      1.00      1.00       375\n",
      "       Good       1.00      1.00      1.00       365\n",
      "\n",
      "avg / total       1.00      1.00      1.00       740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on train data for Random forest model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "### on training data\n",
    "y_train_pred_rf3 = rf_modl.predict(X_train)\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_train, y_train_pred_rf3)))\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred_rf3,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_train)\n",
    "print_confusion_matrix(y_train, y_train_pred_rf3)\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred_rf3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.86\n",
      "[[81  7]\n",
      " [19 79]]\n",
      "True positive =  81\n",
      "False positive =  19\n",
      "False negative =  7\n",
      "True negative =  79\n",
      "Total Cost  44500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Bad       0.81      0.92      0.86        88\n",
      "       Good       0.92      0.81      0.86        98\n",
      "\n",
      "avg / total       0.87      0.86      0.86       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicton and measure on validation data for Random forest model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "#Prediction on Validation data\n",
    "y_pred_validation_rf3 = rf_modl.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_validation, y_pred_validation_rf3)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_validation = confusion_matrix(y_validation, y_pred_validation_rf3,labels=[\"Bad\",\"Good\"])\n",
    "print(confusion_matrix_validation)\n",
    "print_confusion_matrix(y_validation, y_pred_validation_rf3)\n",
    "\n",
    "#Classficattion report on Validaton dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validation, y_pred_validation_rf3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
